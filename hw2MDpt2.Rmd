---
title: "Causal ML Homework 2 - code (part II)"
author: "Luis Armona and Jack Blundell"
date: "May 22, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup

```{r load, echo=F, message=FALSE, warning=FALSE}
# set your working directory

setwd("C:/Users/Jack/Documents/Git/AtheyMLhw2") # Jack
#setwd('/home/luis/AtheyMLhw1') #Luis
# clear things in RStudio

rm(list = ls())


# Call packages

library(ggplot2)
#library(dplyr)
#library(reshape2)
#library(glmnet)
#library(plotmo)
#library(pogs)
#library(balanceHD)
library(causalTree)
library(randomForestCI)
library(reshape2)
library(plyr)
library(gradient.forest)
library(glmnet)


# set seed
set.seed(12345)

fname <- 'Data/charitable_withdummyvariables.csv'
char <- read.csv(fname)

```

Setup our data ready to feed into functions

```{r obs setup, echo=F, message=FALSE, warning=FALSE}

# Extract the dependent variable
Y <- char[["out_amountgive"]]

# Extract treatment
W <- char[["treatment"]]

# Extract covariates
covariates <- char[,c(14:22,23:63)] # inc missing dummies for now
covariate.names <- names(covariates)

# standardize
covariates.scaled <- scale(covariates)
processed.unscaled <- data.frame(Y, W, covariates)
processed.scaled <- data.frame(Y, W, covariates.scaled)

# training, validation, and test sets.
set.seed(46)
smplmain <- sample(nrow(processed.scaled), round(2*nrow(processed.scaled)/3), replace=FALSE)

processed.scaled.train <- processed.scaled[smplmain,]
processed.scaled.test.C <- processed.scaled[-smplmain,]

y.train <- as.matrix(processed.scaled.train$Y, ncol=1)
y.test <- as.matrix(processed.scaled.test.C$Y, ncol=1)

# equal sized sets
smplcausal <- sample(nrow(processed.scaled.train), 
                     round(5*nrow(processed.scaled.train)/10), replace=FALSE)
processed.scaled.train.A <- processed.scaled.train[smplcausal,]
processed.scaled.train.B <- processed.scaled.train[-smplcausal,]

# combination of B and C

processed.scaled.test.B.C <- rbind(processed.scaled.train.B,processed.scaled.test.C)

# as formulas
print(covariate.names)
sumx = paste(covariate.names, collapse = " + ")  # "X1 + X2 + X3 + ..." for substitution later
interx = paste(" (",sumx, ")^2", sep="")  # "(X1 + X2 + X3 + ...)^2" for substitution later

# Y ~ X1 + X2 + X3 + ... 
linearnotreat <- paste("Y",sumx, sep=" ~ ")
linearnotreat <- as.formula(linearnotreat)
linearnotreat

# Y ~ W + X1 + X2 + X3 + ...
linear <- paste("Y",paste("W",sumx, sep=" + "), sep=" ~ ")
linear <- as.formula(linear)
linear

# Y ~ W * (X1 + X2 + X3 + ...)   
# ---> X*Z means include these variables plus the interactions between them
linearhet <- paste("Y", paste("W * (", sumx, ") ", sep=""), sep=" ~ ")
linearhet <- as.formula(linearhet)
linearhet

# Propensity score
processed.scaled.test.C$propens <- mean(processed.scaled.test.C$W)
processed.scaled.test.B.C$propens <- mean(processed.scaled.test.B.C$W)

# Randomized experiment. Use constant propensity score.
# Build Ystar to evaluate methods against

processed.scaled.test.C$Ystar <- processed.scaled.test.C$W * (processed.scaled.test.C$Y/processed.scaled.test.C$propens) -
  (1-processed.scaled.test.C$W) * (processed.scaled.test.C$Y/(1-processed.scaled.test.C$propens))


## Also do this for B.C union

processed.scaled.test.B.C$Ystar <- processed.scaled.test.B.C$W * (processed.scaled.test.B.C$Y/processed.scaled.test.B.C$propens) -
  (1-processed.scaled.test.B.C$W) * (processed.scaled.test.B.C$Y/(1-processed.scaled.test.B.C$propens))

## Some additional datasets useful later


processed.scaled.test.CW0 <- processed.scaled.test.C
processed.scaled.test.CW0$W <- rep(0,nrow(processed.scaled.test.C))

processed.scaled.test.CW1 <- processed.scaled.test.C
processed.scaled.test.CW1$W <- rep(1,nrow(processed.scaled.test.C))

processed.scaled.test.B.CW0 <- processed.scaled.test.B.C
processed.scaled.test.B.CW0$W <- rep(0,nrow(processed.scaled.test.B.C))

processed.scaled.test.B.CW1 <- processed.scaled.test.B.C
processed.scaled.test.B.CW1$W <- rep(1,nrow(processed.scaled.test.B.C))



# Set up MSE vector to record performance

MSElabelvec <- c("")
MSEvec <- c("")

```

Next set some parameters for causal tree / forest

```{r obs setup.params, echo=T}


# causal tree/forest params

# Set parameters
split.Rule.temp = "CT"
cv.option.temp = "CT"
split.Honest.temp = T
cv.Honest.temp = T
split.alpha.temp = .5
cv.alpha.temp = .5
split.Bucket.temp = T
bucketMax.temp= 100
bucketNum.temp = 5
minsize.temp=50


# number of trees (try 1000 once all working)
numtreesCT <- 100
numtreesGF <- 100

```

Use LASSO to estimate heterogeneous causal effects

```{r obs lasso.het, echo=T}

# setup models

#linear.train <- model.matrix(linearhet, processed.scaled.train)[,-1]
linear.test <- model.matrix(linearhet, processed.scaled.test.C)[,-1]
linear.train.1 <- model.matrix(linearhet, processed.scaled.train.A)[,-1]
linear.train.2 <- model.matrix(linearhet, processed.scaled.train.B)[,-1]

# set penalty factor for coef on perbush lower
p.fac = rep(1, 1 + 2*ncol(covariates))
p.fac[c(19,ncol(covariates) + 19)] = 0

# set penalty factor for coef on previous donations low
#p.fac[c(2, ncol(covariates) + 2)] = 0

# Fit lasso on A
lasso.linear <- cv.glmnet(linear.train.1, y.train[smplcausal,],  alpha=1, parallel=TRUE, penalty.factor = p.fac)
#lasso.linear

# plot & select the optimal shrinkage parameter lambda
plot(lasso.linear)
lasso.linear$lambda.min # min average CV error
lasso.linear$lambda.1se #  error within one s.e. of min

# List non-zero coefficients
coef <- predict(lasso.linear, type = "nonzero")
colnames <- colnames(linear.train.1)
selected.vars <- colnames[unlist(coef)]
print(selected.vars)

# set up as formula for post OLS
linearwithlass <- paste("Y", paste(append(selected.vars, "W"),collapse=" + "), sep = " ~ ") 
linearwithlass <- as.formula(linearwithlass)

# OLS using these coefficients on sample A
lm.linear.lasso.A <- lm(linearwithlass, data=processed.scaled.train.A)
summary(lm.linear.lasso.A)

# OLS using these coefficients on sample B
lm.linear.lasso.B <- lm(linearwithlass, data=processed.scaled.train.B)
summary(lm.linear.lasso.B)

# OLS using these coefficients on sample C
lm.linear.lasso.C <- lm(linearwithlass, data=processed.scaled.test.C)
summary(lm.linear.lasso.C)

# OLS using union of samples B and C

lm.linear.lasso.B.C <- lm(linearwithlass, data=processed.scaled.test.B.C)
summary(lm.linear.lasso.B.C)

# Predict on union of B and C for average treatment effect 

predictedW0 <- predict(lm.linear.lasso.B.C, newdata=processed.scaled.test.B.CW0)

predictedW1 <- predict(lm.linear.lasso.B.C, newdata=processed.scaled.test.B.CW1)

lassocauseff <- predictedW1-predictedW0

# calculate MSE against Ystar
lassoMSEstar <- mean((processed.scaled.test.B.C$Ystar-lassocauseff)^2)
print(c("MSE using ystar on test set of lasso",lassoMSEstar))
MSElabelvec <- append(MSElabelvec,"lasso")
MSEvec <- append(MSEvec,lassoMSEstar)


```

# Honest causal tree.

```{r hon.causal.tree, echo=T}

# Set parameters
split.Rule.temp = "CT"
cv.option.temp = "CT"
split.Honest.temp = T
cv.Honest.temp = T
split.alpha.temp = .5
cv.alpha.temp = .5
split.Bucket.temp = T
bucketMax.temp= 100
bucketNum.temp = 5
minsize.temp=50

# honest version (fit on A + B, estimate on C)
CTtree <- honest.causalTree(as.formula(paste("Y~",sumx)), 
                   data=processed.scaled.train, treatment=processed.scaled.train$W, 
                   est_data = processed.scaled.test.C, est_treatment = processed.scaled.test.C$W,
                   split.Rule=split.Rule.temp, split.Honest=split.Honest.temp, 
                   split.Bucket=split.Bucket.temp, bucketNum = bucketNum.temp, 
                   bucketMax = bucketMax.temp, cv.option=cv.option.temp, #cv.Honest=cv.Honest.temp, 
                   minsize = minsize.temp, 
                   split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp, 
                   HonestSampleSize=nrow(processed.scaled.test.C))

opcpid <- which.min(CTtree$cp[,4])
opcp <- CTtree$cp[opcpid,1]
tree_honest_CT_prune <- prune(CTtree, cp = opcp) 
print(tree_honest_CT_prune)

# leaf treatment effects on A+B,A,B,C and B + C

processed.scaled.train$leaffact <- as.factor(round(predict(tree_honest_CT_prune, 
                                        newdata=processed.scaled.train,type="vector"),4))

processed.scaled.train.A$leaffact <- as.factor(round(predict(tree_honest_CT_prune, 
                                        newdata=processed.scaled.train.A,type="vector"),4))

processed.scaled.train.B$leaffact <- as.factor(round(predict(tree_honest_CT_prune, 
                                                             newdata=processed.scaled.train.B,type="vector"),4))

processed.scaled.test.C$leaffact <- as.factor(round(predict(tree_honest_CT_prune, 
                                                             newdata=processed.scaled.test.C,type="vector"),4))

processed.scaled.test.B.C$leaffact <- as.factor(round(predict(tree_honest_CT_prune, 
                                                             newdata=processed.scaled.test.B.C,type="vector"),4))

# These show leaf treatment effects and standard errors; can test hypothesis that leaf 
# treatment effects are 0
summary(lm(Y~leaffact+W*leaffact-W-1, data=processed.scaled.train))
summary(lm(Y~leaffact+W*leaffact-W-1, data=processed.scaled.train.A))
summary(lm(Y~leaffact+W*leaffact-W-1, data=processed.scaled.train.B))
summary(lm(Y~leaffact+W*leaffact-W-1, data=processed.scaled.test.C))
summary(lm(Y~leaffact+W*leaffact-W-1, data=processed.scaled.test.B.C))

# Test whether leaf treatment effects are different than average

summary(lm(Y~leaffact+W*leaffact-1, data=processed.scaled.test.C))

# Predict using C

CTpredict = predict(tree_honest_CT_prune, newdata=processed.scaled.test.C, type="vector")

# calculate MSE against Ystar

CTMSEstar <- mean((processed.scaled.test.C$Ystar-CTpredict)^2)
print(c("MSE using ystar on test set of single tree",CTMSEstar))
MSElabelvec <- append(MSElabelvec,"causal tree")
MSEvec <- append(MSEvec,CTMSEstar)

```

 Now lets use causalForest from the causalTree package

```{r obs causal.f, echo=T}

ncolx<-length(processed.scaled.train)-3 # number of covariates
ncov_sample<-floor(2*ncolx/3) #number of covariates (randomly sampled) to use to build tree
# ncov_sample<-p #use this line if all covariates need to be used in all trees

# set seed
set.seed(123)
# now estimate a causalForest. Train on A and B.
cf <- causalForest(as.formula(paste("Y~",sumx)), data=processed.scaled.train, 
                   treatment=processed.scaled.train$W, 
                   split.Rule="CT", double.Sample = T, split.Honest=T,  split.Bucket=T, 
                   bucketNum = 5,
                   bucketMax = 100, cv.option="CT", cv.Honest=T, minsize = 50, 
                   split.alpha = 0.5, cv.alpha = 0.5,
                   sample.size.total = floor(nrow(processed.scaled.train) / 2), 
                   sample.size.train.frac = .5,
                   mtry = ncov_sample, nodesize = 5, 
                   num.trees= numtreesCT,ncolx=ncolx,ncov_sample=ncov_sample) 

# Test on C.
cfpredtest <- predict(cf, newdata=processed.scaled.test.C, type="vector")
print(cfpredtest)

cfpredtrainall <- predict(cf, newdata=processed.scaled.train, 
                          predict.all = TRUE, type="vector")

# MSE against Ystar
cfMSEstar <- mean((processed.scaled.test.C$Ystar-cfpredtest)^2)
print(c("MSE using ystar on test set of causalTree/causalForest",cfMSEstar))
mean(cfMSEstar)

# ATE
print(c("mean of ATE treatment effect from causalForest on Training data", 
        round(mean(cfpredtrainall$aggregate),5)))

print(c("mean of ATE treatment effect from causalForest on Test data", 
        round(mean(cfpredtest),5)))


# use infJack routine from randomForestCI
# This gives variances for each of the estimated treatment effects; note tau is labelled y.hat
cfvar <- infJack(cfpredtrainall$individual, cf$inbag, calibrate = TRUE)
plot(cfvar)


# plot in two dimensions while holding others at their medians

processed.scaled.train$leaffact <- NULL # drop this temporary var

namesD <- names(processed.scaled.train)
D = as.matrix(processed.scaled.train)
medians = apply(D, 2, median)

unique.hpa = sort(unique(as.numeric(D[,"hpa"])))
unique.perbush = sort(unique(as.numeric(D[,"perbush"])))
unique.vals = expand.grid(hpa = unique.hpa, perbush = unique.perbush)

D.focus = outer(rep(1, nrow(unique.vals)), medians)
D.focus[,"hpa"] = unique.vals[,"hpa"]
D.focus[,"perbush"] = unique.vals[,"perbush"]
D.focus = data.frame(D.focus)
numcol = ncol(D.focus)
names(D.focus) = namesD

direct.df = expand.grid(hpa=factor(unique.hpa), perbush=factor(unique.perbush))
direct.df$cate=  predict(cf, newdata=D.focus, type="vector", predict.all=F)

#heatmapdata <- direct.df
#heatmapdata <- heatmapdata[,c("hpa","perbush","cate")]
#heatmapdata <- heatmapdata[order(heatmapdata$hpa),]
#heatmapdata <- dcast(heatmapdata, hpa~perbush, mean)#

#heatmapdata <- heatmapdata[,!(names(heatmapdata) %in% c("hpa"))]

#need to remove the labels from this heatmap--to do
#heatmap(as.matrix(heatmapdata), Rowv=NA, Colv=NA, col = cm.colors(256), scale="column", margins=c(5,10),
#        labCol<-rep("",ncol(heatmapdata)), labRow<-rep("",nrow(heatmapdata)))


# gg plot
ggplot(direct.df, aes(hpa,perbush)) + geom_tile(aes(fill = cate)) 


```

Now lets use the gradient.forest package

```{r gradient.forest, echo=T}

# label some variables
X = as.matrix(processed.scaled.train[,covariate.names])
X.test = as.matrix(processed.scaled.test.C[,covariate.names])
Y  = as.matrix(processed.scaled.train[,"Y"])
W  = as.matrix(processed.scaled.train[,"W"])

# run gradient forest
gf <- causal.forest(X, Y, W, num.trees = numtreesGF, ci.group.size = 4,
                    precompute.nuisance = FALSE)

# Predict on training and test sets
preds.causal.oob = predict(gf, estimate.variance=TRUE)
preds.causal.test = predict(gf, X.test, estimate.variance=TRUE)
mean(preds.causal.oob$predictions)  
plot(preds.causal.oob$predictions, preds.causal.oob$variance.estimates)


mean(preds.causal.test$predictions)  
plot(preds.causal.test$predictions, preds.causal.test$variance.estimates)


# calculate MSE against Ystar
gfMSEstar <- mean((processed.scaled.test.C$Ystar-preds.causal.test$predictions)^2)
print(c("MSE using ystar on test set of gradient causal forest",gfMSEstar))
MSElabelvec <- append(MSElabelvec,"gradient causal forest")
MSEvec <- append(MSEvec,gfMSEstar)

# Now lets try with orthogonalization of Y (not W since random assignment)

Yrf <- regression.forest(X, Y, num.trees = numtreesGF, ci.group.size = 4)
Yresid <- Y - predict(Yrf)$prediction


gfr <- causal.forest(X,Yresid,W,num.trees=numtreesGF, ci.group.size=4, 
                     precompute.nuisance = FALSE)
preds.causalr.oob = predict(gfr, estimate.variance=TRUE)
mean(preds.causalr.oob$predictions)  
plot(preds.causalr.oob$predictions, preds.causal.oob$variance.estimates)

Xtest = as.matrix(processed.scaled.test.C[,covariate.names])
preds.causalr.test = predict(gfr, Xtest, estimate.variance=TRUE)
mean(preds.causalr.test$predictions)  
plot(preds.causalr.test$predictions, preds.causal.test$variance.estimates)

# calculate MSE against Ystar
gfrMSEstar <- mean((processed.scaled.test.C$Ystar-preds.causalr.test$predictions)^2)
print(c("MSE using ystar on test set of orth causal gradient forest",gfrMSEstar))
MSElabelvec <- append(MSElabelvec,"orth causal gradient forest")
MSEvec <- append(MSEvec,gfrMSEstar)

```
