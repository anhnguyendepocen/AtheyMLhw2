---
title: "Causal ML Homework 2 - code (part I)"
author: "Luis Armona and Jack Blundell"
date: "May 22, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup

```{r load, echo=F, message=FALSE, warning=FALSE}
# set your working directory

setwd("C:/Users/Jack/Documents/Git/AtheyMLhw2") # Jack
#setwd('/home/luis/AtheyMLhw1') #Luis
# clear things in RStudio

rm(list = ls())


# Call packages

library(ggplot2)
#library(dplyr)
#library(reshape2)
#library(glmnet)
#library(plotmo)
#library(pogs)
#library(balanceHD)
library(causalTree)
library(randomForestCI)
library(reshape2)
library(plyr)
library(gradient.forest)



# set seed
set.seed(12345)

fname <- 'Data/charitable_withdummyvariables.csv'
char <- read.csv(fname)

```

First drop observations as in HW1 to make this into an observational study

```{r drop, echo=TRUE}

#randomly censor individuals
# via a  complex, highly nonlinear fcn 
#

ps.fcn <- function(v,c,pg,t){
  #v_t <- (v-.25)/.5
  v_t <- v
  #ihs_pg <- log(pg + sqrt(pg ^ 2 + 1))/5
  #p<- (c*(acos(v_t))*atan(v_t^2)  - .5*exp(v_t))/4 + (t*((ihs_pg)) + (1-t))/2
  ihs_pg <- log(pg + sqrt(pg ^ 2 + 1))
  p<- (1-t)*(c+1)*(acos(v_t)*atan(v_t) )/3 + 
      t*(.01+(-.01*ihs_pg^5 + 1*ihs_pg^3)/300)
  p<- pmin(pmax(0,p),1)
  return(p)
}
#story to accompany this fcn: ACLU wants to help those in trouble in "red states" but do not 
#feel they can make a difference in really, really red states so target donors less often


# Selection rule
char$ps.select <- ps.fcn(char$perbush,char$cases,char$hpa,char$treatment) # hpa is highest previous contribution. cases is court cases from state which organization was involved.
#deal with those missing covariates
char$ps.select[ which(char$perbush==-999
            | char$cases==-999
            | char$hpa==-999)] <- 0.5

# Set seed
set.seed(21) 

#replace -999s with 0s (since there are already missing dummies)
for (v in names(char)){
  mi_v <- paste(v,'_missing',sep='') 
  if (mi_v %in% names(char)){
    print(paste('fixing',v))
    char[(char[,mi_v]==1),v]<-0
  }
}

# Selection rule (=1 of uniform random [0,1] is lower, so those with higher ps.true more likely to be selected)
selection <- runif(nrow(char)) <= char$ps.select

char.censored <- char[selection,] #remove observations via propensity score rule


```

Further setup our data ready to feed into functions.

```{r obs setup.obs, echo=T}

# Extract the dependent variable
Y <- char.censored[["out_amountgive"]]

# Extract treatment
W <- char.censored[["treatment"]]

# Extract covariates
covariates <- char.censored[,c(14:22,23:63)] # inc missing dummies for now
covariate.names <- names(covariates)

# standardize
covariates.scaled <- scale(covariates)
processed.unscaled <- data.frame(Y, W, covariates)
processed.scaled <- data.frame(Y, W, covariates.scaled)

# training and test sets.
set.seed(44)
smplmain <- sample(nrow(processed.scaled), round(9*nrow(processed.scaled)/10), replace=FALSE)

processed.scaled.train <- processed.scaled[smplmain,]
processed.scaled.test <- processed.scaled[-smplmain,]

y.train <- as.matrix(processed.scaled.train$Y, ncol=1)
y.test <- as.matrix(processed.scaled.test$Y, ncol=1)

# as formulas
print(covariate.names)
sumx = paste(covariate.names, collapse = " + ")  # "X1 + X2 + X3 + ..." for substitution later
interx = paste(" (",sumx, ")^2", sep="")  # "(X1 + X2 + X3 + ...)^2" for substitution later

```

Next set some parameters for causal tree / forest

```{r obs setup.params, echo=T}
# Create additional datasets

processed.scaled.testW0 <- processed.scaled.test
processed.scaled.testW0$W <- rep(0,nrow(processed.scaled.test))

processed.scaled.testW1 <- processed.scaled.test
processed.scaled.testW1$W <- rep(1,nrow(processed.scaled.test))

# causal tree/forest params

# Set parameters
split.Rule.temp = "CT"
cv.option.temp = "CT"
split.Honest.temp = T
cv.Honest.temp = T
split.alpha.temp = .5
cv.alpha.temp = .5
split.Bucket.temp = T
bucketMax.temp= 100
bucketNum.temp = 5
minsize.temp=50


# number of trees (try 1000 once all working)
numtreesCT <- 1000
numtreesGF <- 1000

```

Now lets try propensity forest. Remember no honest estimation here.

```{r obs propens.forest, echo=F}


# Propensity forest

ncolx<-length(processed.scaled.train)-2 #total number of covariates
ncov_sample<-floor(ncolx/3) #number of covariates (randomly sampled) to use to build tree

pf <- propensityForest(as.formula(paste("Y~",sumx)), 
                       data=processed.scaled.train,
                       treatment=processed.scaled.train$W, 
                       split.Bucket=F, 
                       sample.size.total = floor(nrow(processed.scaled.train) / 2), 
                       nodesize = 25, num.trees=numtreesCT,
                       mtry=ncov_sample, ncolx=ncolx, ncov_sample=ncov_sample )

pfpredtest <- predict(pf, newdata=processed.scaled.test, type="vector")

pfpredtrainall <- predict(pf, newdata=processed.scaled.train, 
                          predict.all = TRUE, type="vector")
print(c("mean of ATE treatment effect from propensityForest on Training data", 
        round(mean(pfpredtrainall$aggregate),5)))

pfvar <- infJack(pfpredtrainall$individual, pf$inbag, calibrate = TRUE)
plot(pfvar)

```

# Now lets try gradient forest, first without residualizing

```{r obs gradient.forest, echo=T}

X = as.matrix(processed.scaled.train[,covariate.names])
X.test = as.matrix(processed.scaled.test[,covariate.names])
Y  = as.matrix(processed.scaled.train[,"Y"])
W  = as.matrix(processed.scaled.train[,"W"])

gf <- causal.forest(X, Y, W, num.trees = numtreesGF, ci.group.size = 4,
                    precompute.nuisance = FALSE)
preds.causal.oob = predict(gf, estimate.variance=TRUE)
preds.causal.test = predict(gf, X.test, estimate.variance=TRUE)

# training set
mean(preds.causal.oob$predictions)  
plot(preds.causal.oob$predictions, preds.causal.oob$variance.estimates)

# test set (honest)
mean(preds.causal.test$predictions)  
plot(preds.causal.test$predictions, preds.causal.test$variance.estimates)

```

# Next the same but residualizing

```{r obs gradient.forest.res, echo=T}

# orthogonalize Y
Yrf <- regression.forest(X, Y, num.trees = numtreesGF, ci.group.size = 4)
Yresid <- Y - predict(Yrf)$prediction

# orthogonalize W 
Wrf <- regression.forest(X, W, num.trees = numtreesGF, ci.group.size = 4)
Wresid <- W - predict(Wrf)$predictions


gfr <- causal.forest(X,Yresid,Wresid,num.trees=numtreesGF, ci.group.size=4, 
                     precompute.nuisance = FALSE)

# training set
preds.causalr.oob = predict(gfr, estimate.variance=TRUE)
mean(preds.causalr.oob$predictions)  
plot(preds.causalr.oob$predictions, preds.causal.oob$variance.estimates)

# test set (honest)
Xtest = as.matrix(processed.scaled.test[,covariate.names])
preds.causalr.test = predict(gfr, Xtest, estimate.variance=TRUE)
mean(preds.causalr.test$predictions)  
plot(preds.causalr.test$predictions, preds.causal.test$variance.estimates)


```
