\documentclass[paper=letter, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height
\setlength{\parskip}{1em}
\setlength{\parindent}{3em}
\usepackage{listings}

\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{geometry}
 \geometry{
 bottom=1in,
 right=1in,
 left=1in,
 top=1in,
 }

 \usepackage[flushleft]{threeparttable}
\usepackage[capposition=top]{floatrow}

\usepackage{graphicx}
 \graphicspath{ {../figures/} }

\title{	
\normalfont \normalsize 
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
 \large{{\textbf{ECON 293 Homework 2: Commentary}}} \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{\small{Jack Blundell, Spring 2017}} % Your name

\date{} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

In this commentary I discuss results and include some key figures. Many further figures and all code are provided in the attached .html file. I worked with Luis Armona on the code. This commentary is written individually.

As in the previous homework, we use data from ``Does Price Matter in Charitable Giving? Evidence from a Large-Scale Natural Field Experiment", by Dean Karlan and John List (AER 2007). In this experiment, two-thirds of recipients of a charity solicitation letter receive some kind of match-donation treatment, whereas the remaining control group receives the letter alone, with no matching promise. We use the same censoring rule as in homework 1 to eliminate observations, emulating an observational study for the first part of this homework.

\section{Observational study}

This first set of methods is applied to our censored data, emulating observational data with non-random treatment assignment.

\subsection{Propensity forest}

Our first method is propensity forest from package causalTree. In this method, we use a random forest where in each tree we split the sample into leaves based on propensity score, then estimate the treatment effect within each leaf. This is then a fairly natural extension of propensity score matching using machine learning methods. We can summarise the estimates with the average treatment effect, calculated by averaging over all estimated treatment effects. In doing so, we get an estimate of $0.2661$. Relative to our estimates in homework 1...


\subsection{Gradient forest}

Gradient forests (Athey, Tibshirani and Wager 2016) allows the estimation of heterogeneous parameters at a particular point in covariate space by using 'neighborhoods' of observations in a training set. Many existing methods to do this suffer from the curse of dimensionality and are extremely computationally intensive. Gradient forests solves many of these computational problems, using an adaptive weighting function derived from random forests. When fitting trees, observations are labeled with the gradient of the estimating equation. Next, as is standard when using trees, observations are split into leaves and parameters of interest are estimated within leaves. Importantly, it has been shown that this method delivers consistent, asymptotically normal estimates.

Using gradient forest on the raw data, we obtain an estimated ATE of $ $.

As an alternative, we first residualize our data. To do so, we use a random forest to predict our outcome and treatment variables, based on covariates. We then subtract these predicted values to obtain residuals. With these residuals of $Y$ and $W$ we then run gradient forest just as above. We obtain an estimated ATE of $ $.

Comparing to our other estimates, we see that...


\section{Randomized trial}

\subsection{LASSO}

\subsection{Honest causal tree}

\subsection{Causal forest}

\subsection{Causal forest (Gradient forest)}

\end{document}
