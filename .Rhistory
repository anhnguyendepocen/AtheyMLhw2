linearwithlass <- paste("Y", paste(append(selected.vars, "W"),collapse=" + "), sep = " ~ ")
linearwithlass <- as.formula(linearwithlass)
lm.linear.lasso <- lm(linearwithlass, data=processed.scaled.train.2)
yhat.linear.lasso <- predict(lm.linear.lasso, newdata=processed.scaled.test)
summary(lm.linear.lasso)
predictedW0 <- predict(lm.linear.lasso, newdata=processed.scaled.testW0)
predictedW1 <- predict(lm.linear.lasso, newdata=processed.scaled.testW1)
lassocauseff <- predictedW1-predictedW0
# calculate MSE against Ystar
lassoMSEstar <- mean((processed.scaled.test$Ystar-lassocauseff)^2)
print(c("MSE using ystar on test set of lasso",lassoMSEstar))
MSElabelvec <- append(MSElabelvec,"lasso")
MSEvec <- append(MSEvec,lassoMSEstar)
lm.linear.lasso <- lm(linearwithlass, data=processed.scaled.train.2)
yhat.linear.lasso <- predict(lm.linear.lasso, newdata=processed.scaled.test)
summary(lm.linear.lasso)
summary(lm.linear.test)
lm.linear.lasso.test <- lm(linearwithlass, data=processed.scaled.test)
summary(lm.linear.test)
summary(lm.linear.lasso.test)
B.C.union <- append(processed.scaled.test,processed.scaled.train.2)
lm.linear.lasso.test <- lm(linearwithlass, data=B.C.union)
B.C.union <- append(processed.scaled.test,processed.scaled.train.2)
lm.linear.lasso.test <- lm(linearwithlass, data=B.C.union)
B.C.union <- rbind(processed.scaled.test,processed.scaled.train.2)
ncol(processed.scaled.test)
ncol(processed.scaled.train.2)
names(processed.scaled.train.2)
names(processed.scaled.test)
C <- processed.scaled.test
C$propens <- NULL
C$Ystar <- NULL
lm.linear.lasso.test <- lm(linearwithlass, data=C)
summary(lm.linear.lasso.test)
B.C <- rbind(processed.scaled.test, C)
B.C <- rbind(processed.scaled.train.2, C)
lm.linear.lasso.test <- lm(linearwithlass, data=B.C)
summary(lm.linear.lasso.test)
ncolx<-length(processed.scaled.train)-2 #total number of covariates
ncov_sample<-floor(2*ncolx/3) #number of covariates (randomly sampled) to use to build tree
cf <- causalForest(as.formula(paste("Y~",sumx)), data=processed.scaled.train,
treatment=processed.scaled.train$W,
split.Rule="CT", double.Sample = T, split.Honest=T,  split.Bucket=T,
bucketNum = 5,
bucketMax = 100, cv.option="CT", cv.Honest=T, minsize = 50,
split.alpha = 0.5, cv.alpha = 0.5,
sample.size.total = floor(nrow(processed.scaled.train) / 2),
sample.size.train.frac = .5,
mtry = ncov_sample, nodesize = 5,
num.trees= numtreesCT,ncolx=ncolx,ncov_sample=ncov_sample
)
cfpredtest <- predict(cf, newdata=processed.scaled.test, type="vector")
cfpredtrainall <- predict(cf, newdata=processed.scaled.train,
predict.all = TRUE, type="vector")
cfpredtest
cfMSEstar <- mean((processed.scaled.test$Ystar-cfpredtest)^2)
print(c("MSE using ystar on test set of causalTree/causalForest",cfMSEstar))
mean(cfMSEstar)
print(c("mean of ATE treatment effect from causalForest on Training data",
round(mean(cfpredtrainall$aggregate),5)))
print(c("mean of ATE treatment effect from causalForest on Test data",
round(mean(cfpredtest),5)))
cfvar <- infJack(cfpredtrainall$individual, cf$inbag, calibrate = TRUE)
cfvar <- infJack(cfpredtrainall$individual, cf$inbag, calibrate = TRUE)
plot(cfvar)
namesD <- names(processed.scaled.train)
D = as.matrix(processed.scaled.train)
medians = apply(D, 2, median)
unique.yob = sort(unique(as.numeric(D[,"yob"])))
unique.yob = sort(unique(as.numeric(D[,"hpa"])))
unique.totalpopulation_estimate = sort(unique(as.numeric(D[,"perbush"])))
namesD <- names(processed.scaled.train)
D = as.matrix(processed.scaled.train)
medians = apply(D, 2, median)
unique.hpa = sort(unique(as.numeric(D[,"hpa"])))
unique.perbush = sort(unique(as.numeric(D[,"perbush"])))
unique.vals = expand.grid(hpa = unique.hpa, perbush = unique.perbush)
D.focus = outer(rep(1, nrow(unique.vals)), medians)
D.focus[,"hpa"] = unique.vals[,"hpa"]
D.focus[,"perbush"] = unique.vals[,"perbush"]
D.focus = data.frame(D.focus)
numcol = ncol(D.focus)
names(D.focus) = namesD
direct.df = expand.grid(hpa=factor(unique.hpa), perbush=factor(unique.perbush))
direct.df$cate=  predict(cf, newdata=D.focus, type="vector", predict.all=FALSE)
direct.df$cate=  predict(cf, newdata=D.focus, type="vector", predict.all=FALSE)
heatmapdata <- direct.df
heatmapdata <- heatmapdata[,c("hpa","perbush","cate")]
heatmapdata <- heatmapdata[order(heatmapdata$hpa),]
heatmapdata <- dcast(heatmapdata, hpa~perbush, mean)
direct.df$cate=  predict(cf, newdata=D.focus, type="vector", predict.all=FALSE)
heatmapdata <- direct.df
heatmapdata <- heatmapdata[,c("hpa","perbush","cate")]
heatmapdata <- heatmapdata[order(heatmapdata$hpa),]
heatmapdata <- dcast(heatmapdata, hpa~perbush, mean)
heatmapdata <- heatmapdata[,!(names(heatmapdata) %in% c("hpa"))]
heatmapdata <- heatmapdata[,!(names(heatmapdata) %in% c("hpa"))]
heatmap(as.matrix(heatmapdata), Rowv=NA, Colv=NA, col = cm.colors(256), scale="column", margins=c(5,10),
labCol<-rep("",ncol(heatmapdata)), labRow<-rep("",nrow(heatmapdata)))
heatmap(as.matrix(heatmapdata), Rowv=NA, Colv=NA, col = cm.colors(256), scale="column", margins=c(5,10),
labCol<-rep("",ncol(heatmapdata)), labRow<-rep("",nrow(heatmapdata)))
ggplot(direct.df, aes(hpa,perbush)) + geom_tile(aes(fill = cate))
?causal.forest
unique.vals = expand.grid(hpa = unique.hpa, perbush = unique.perbush)
X = as.matrix(processed.scaled.train[,covariate.names])
X.test = as.matrix(processed.scaled.test[,covariate.names])
Y  = as.matrix(processed.scaled.train[,"Y"])
X = as.matrix(processed.scaled.train[,covariate.names])
X.test = as.matrix(processed.scaled.test[,covariate.names])
Y  = as.matrix(processed.scaled.train[,"Y"])
W  = as.matrix(processed.scaled.train[,"W"])
gf <- causal.forest(X, Y, W, num.trees = numtreesGF, ci.group.size = 4,
precompute.nuisance = FALSE)
preds.causal.oob = predict(gf, estimate.variance=TRUE)
preds.causal.test = predict(gf, X.test, estimate.variance=TRUE)
mean(preds.causal.oob$predictions)
plot(preds.causal.oob$predictions, preds.causal.oob$variance.estimates)
mean(preds.causal.test$predictions)
plot(preds.causal.test$predictions, preds.causal.test$variance.estimates)
# calculate MSE against Ystar
gfMSEstar <- mean((processed.scaled.test$Ystar-preds.causal.test$predictions)^2)
print(c("MSE using ystar on test set of gradient causal forest",gfMSEstar))
MSElabelvec <- append(MSElabelvec,"gradient causal forest")
MSEvec <- append(MSEvec,gfMSEstar)
# Now try with orthogonalization--orthog does not have to be random forest
Yrf <- regression.forest(X, Y, num.trees = numtreesGF, ci.group.size = 4)
Yresid <- Y - predict(Yrf)$prediction
# orthogonalize W -- if obs study
Wrf <- regression.forest(X, W, num.trees = numtreesGF, ci.group.size = 4)
Wresid <- W - predict(Wrf)$predictions # use if you are orthogonalizing W, e.g. for obs study
gfr <- causal.forest(X,Yresid,Wresid,num.trees=numtreesGF, ci.group.size=4,
precompute.nuisance = FALSE)
preds.causalr.oob = predict(gfr, estimate.variance=TRUE)
mean(preds.causalr.oob$predictions)
plot(preds.causalr.oob$predictions, preds.causal.oob$variance.estimates)
Xtest = as.matrix(processed.scaled.test[,covariate.names])
preds.causalr.test = predict(gfr, Xtest, estimate.variance=TRUE)
mean(preds.causalr.test$predictions)
plot(preds.causalr.test$predictions, preds.causal.test$variance.estimates)
# calculate MSE against Ystar
gfrMSEstar <- mean((processed.scaled.test$Ystar-preds.causalr.test$predictions)^2)
print(c("MSE using ystar on test set of orth causal gradient forest",gfrMSEstar))
MSElabelvec <- append(MSElabelvec,"orth causal gradient forest")
MSEvec <- append(MSEvec,gfrMSEstar)
knitr::opts_chunk$set(echo = TRUE)
# set your working directory
setwd("C:/Users/Jack/Documents/Git/AtheyMLhw2") # Jack
#setwd('/home/luis/AtheyMLhw1') #Luis
# clear things in RStudio
rm(list = ls())
# Call packages
library(ggplot2)
#library(dplyr)
#library(reshape2)
#library(glmnet)
#library(plotmo)
#library(pogs)
#library(balanceHD)
library(causalTree)
library(randomForestCI)
library(reshape2)
library(plyr)
library(gradient.forest)
# set seed
set.seed(12345)
fname <- 'Data/charitable_withdummyvariables.csv'
char <- read.csv(fname)
#randomly censor individuals
# via a  complex, highly nonlinear fcn  of votes 4 bush in state,
#
ps.fcn <- function(v,c,pg,t){
#v_t <- (v-.25)/.5
v_t <- v
#ihs_pg <- log(pg + sqrt(pg ^ 2 + 1))/5
#p<- (c*(acos(v_t))*atan(v_t^2)  - .5*exp(v_t))/4 + (t*((ihs_pg)) + (1-t))/2
ihs_pg <- log(pg + sqrt(pg ^ 2 + 1))
p<- (1-t)*(c+1)*(acos(v_t)*atan(v_t) )/3 +
t*(.01+(-.01*ihs_pg^5 + 1*ihs_pg^3)/300)
p<- pmin(pmax(0,p),1)
return(p)
}
#story to accompany this fcn: ACLU wants to help those in trouble in "red states" but do not
#feel they can make a difference in really, really red states so target donors less often
# Selection rule
char$ps.select <- ps.fcn(char$perbush,char$cases,char$hpa,char$treatment) # hpa is highest previous contribution. cases is court cases from state which organization was involved.
#deal with those missing covariates
char$ps.select[ which(char$perbush==-999
| char$cases==-999
| char$hpa==-999)] <- 0.5
# Set seed
set.seed(21)
#replace -999s with 0s (since there are already missing dummies)
for (v in names(char)){
mi_v <- paste(v,'_missing',sep='')
if (mi_v %in% names(char)){
print(paste('fixing',v))
char[(char[,mi_v]==1),v]<-0
}
}
# Selection rule (=1 of uniform random [0,1] is lower, so those with higher ps.true more likely to be selected)
selection <- runif(nrow(char)) <= char$ps.select
char.censored <- char[selection,] #remove observations via propensity score rule
# Extract the dependent variable
Y <- char.censored[["out_amountgive"]]
# Extract treatment
W <- char.censored[["treatment"]]
# Extract covariates
covariates <- char.censored[,c(14:22,23:44)]
covariate.names <- names(covariates)
# standardize
covariates.scaled <- scale(covariates)
processed.unscaled <- data.frame(Y, W, covariates)
processed.scaled <- data.frame(Y, W, covariates.scaled)
# training, validation, and test sets.
set.seed(44)
smplmain <- sample(nrow(processed.scaled), round(9*nrow(processed.scaled)/10), replace=FALSE)
processed.scaled.train <- processed.scaled[smplmain,]
processed.scaled.test <- processed.scaled[-smplmain,]
y.train <- as.matrix(processed.scaled.train$Y, ncol=1)
y.test <- as.matrix(processed.scaled.test$Y, ncol=1)
# 45-45-10 sample
smplcausal <- sample(nrow(processed.scaled.train),
round(5*nrow(processed.scaled.train)/10), replace=FALSE)
processed.scaled.train.1 <- processed.scaled.train[smplcausal,]
processed.scaled.train.2 <- processed.scaled.train[-smplcausal,]
# as formulas
print(covariate.names)
sumx = paste(covariate.names, collapse = " + ")  # "X1 + X2 + X3 + ..." for substitution later
interx = paste(" (",sumx, ")^2", sep="")  # "(X1 + X2 + X3 + ...)^2" for substitution later
# Y ~ X1 + X2 + X3 + ...
linearnotreat <- paste("Y",sumx, sep=" ~ ")
linearnotreat <- as.formula(linearnotreat)
linearnotreat
# Y ~ W + X1 + X2 + X3 + ...
linear <- paste("Y",paste("W",sumx, sep=" + "), sep=" ~ ")
linear <- as.formula(linear)
linear
# Y ~ W * (X1 + X2 + X3 + ...)
# ---> X*Z means include these variables plus the interactions between them
linearhet <- paste("Y", paste("W * (", sumx, ") ", sep=""), sep=" ~ ")
linearhet <- as.formula(linearhet)
linearhet
#### LOOK AT THIS AGAIN! (note that this is not used in part I))
##### NEED TO ADJUST PROPENSITY SCORE!
processed.scaled.test$propens <- mean(processed.scaled.test$W) #note this is randomized experiment so will use constant propens
processed.scaled.test$Ystar <- processed.scaled.test$W * (processed.scaled.test$Y/processed.scaled.test$propens) -
(1-processed.scaled.test$W) * (processed.scaled.test$Y/(1-processed.scaled.test$propens))
MSElabelvec <- c("")
MSEvec <- c("")
# causal tree/forest params
# Set parameters
split.Rule.temp = "CT"
cv.option.temp = "CT"
split.Honest.temp = T
cv.Honest.temp = T
split.alpha.temp = .5
cv.alpha.temp = .5
split.Bucket.temp = T
bucketMax.temp= 100
bucketNum.temp = 5
minsize.temp=50
# Some of the models need to get causal effects out by comparing mu(X,W=1)-mu(X,W=0).  Create datasets to do that easily
processed.scaled.testW0 <- processed.scaled.test
processed.scaled.testW0$W <- rep(0,nrow(processed.scaled.test))
processed.scaled.testW1 <- processed.scaled.test
processed.scaled.testW1$W <- rep(1,nrow(processed.scaled.test))
# make this bigger -- say 2000 -- if run time permits
numtreesCT <- 200
numtreesGF <- 200
ncolx<-length(processed.scaled.train)-2 #total number of covariates
ncov_sample<-floor(ncolx/3) #number of covariates (randomly sampled) to use to build tree
pf <- propensityForest(as.formula(paste("Y~",sumx)),
data=processed.scaled.train,
treatment=processed.scaled.train$W,
split.Bucket=F,
sample.size.total = floor(nrow(processed.scaled.train) / 2),
nodesize = 25, num.trees=numtreesCT,
mtry=ncov_sample, ncolx=ncolx, ncov_sample=ncov_sample )
pfpredtest <- predict(pf, newdata=processed.scaled.test, type="vector")
pfpredtrainall <- predict(pf, newdata=processed.scaled.train,
predict.all = TRUE, type="vector")
print(c("mean of ATE treatment effect from propensityForest on Training data",
round(mean(pfpredtrainall$aggregate),5)))
?propensityForest
dim(pfpredicttrainall)
dim(pfpredtrainall)
length(pfpredtrainall)
pfpredtrainall
pfpredtrainall
print(c("mean of ATE treatment effect from propensityForest on Training data",
round(mean(pfpredtrainall$aggregate),5)))
Yrf <- regression.forest(X, Y, num.trees = numtreesGF, ci.group.size = 4)
Yresid <- Y - predict(Yrf)$prediction
Yrf <- regression.forest(X, Y, num.trees = numtreesGF, ci.group.size = 4)
# set your working directory
setwd("C:/Users/Jack/Documents/Git/AtheyMLhw2") # Jack
#setwd('/home/luis/AtheyMLhw1') #Luis
# clear things in RStudio
rm(list = ls())
# Call packages
library(ggplot2)
#library(dplyr)
#library(reshape2)
#library(glmnet)
#library(plotmo)
#library(pogs)
#library(balanceHD)
library(causalTree)
library(randomForestCI)
library(reshape2)
library(plyr)
library(gradient.forest)
# set seed
set.seed(12345)
fname <- 'Data/charitable_withdummyvariables.csv'
char <- read.csv(fname)
knitr::opts_chunk$set(echo = TRUE)
# set your working directory
setwd("C:/Users/Jack/Documents/Git/AtheyMLhw2") # Jack
#setwd('/home/luis/AtheyMLhw1') #Luis
# clear things in RStudio
rm(list = ls())
# Call packages
library(ggplot2)
#library(dplyr)
#library(reshape2)
#library(glmnet)
#library(plotmo)
#library(pogs)
#library(balanceHD)
library(causalTree)
library(randomForestCI)
library(reshape2)
library(plyr)
library(gradient.forest)
# set seed
set.seed(12345)
fname <- 'Data/charitable_withdummyvariables.csv'
char <- read.csv(fname)
covars.all <- char[,c(14:22,23:63)] # skip the state indicator used for summ stats
# formula to interact all covariates no interactions for missing dummies.
# for tractability, we interact individ. covars with each other, and state vars with each other
# create design matrix storing all features
covars.regular <-char[,c(14:22,23:44)]
covars.missing <- char[,c(45:63)]
int.level = 2 #the degree of interaction between covariates that are not missing dummies
covars.poly.str = paste('(', paste(names(covars.regular)[1:9],collapse='+'),')^',int.level,
' + (', paste(names(covars.regular)[11:31],collapse='+'),')^',int.level,
' + ',paste(names(covars.missing),collapse='+'),sep='')
#covars.poly.str = paste('(', paste(names(covars.regular),collapse='+'),')^',int.level,
#                        ' + ',paste(names(covars.missing),collapse='+'),sep='')
covars.poly <-model.matrix(as.formula(paste('~ ',covars.poly.str)),data=char)
#randomly censor individuals
# via a  complex, highly nonlinear fcn  of votes 4 bush in state,
#
ps.fcn <- function(v,c,pg,t){
#v_t <- (v-.25)/.5
v_t <- v
#ihs_pg <- log(pg + sqrt(pg ^ 2 + 1))/5
#p<- (c*(acos(v_t))*atan(v_t^2)  - .5*exp(v_t))/4 + (t*((ihs_pg)) + (1-t))/2
ihs_pg <- log(pg + sqrt(pg ^ 2 + 1))
p<- (1-t)*(c+1)*(acos(v_t)*atan(v_t) )/3 +
t*(.01+(-.01*ihs_pg^5 + 1*ihs_pg^3)/300)
p<- pmin(pmax(0,p),1)
return(p)
}
#story to accompany this fcn: ACLU wants to help those in trouble in "red states" but do not
#feel they can make a difference in really, really red states so target donors less often
# Selection rule
char$ps.select <- ps.fcn(char$perbush,char$cases,char$hpa,char$treatment) # hpa is highest previous contribution. cases is court cases from state which organization was involved.
#deal with those missing covariates
char$ps.select[ which(char$perbush==-999
| char$cases==-999
| char$hpa==-999)] <- 0.5
# Set seed
set.seed(21)
#replace -999s with 0s (since there are already missing dummies)
for (v in names(char)){
mi_v <- paste(v,'_missing',sep='')
if (mi_v %in% names(char)){
print(paste('fixing',v))
char[(char[,mi_v]==1),v]<-0
}
}
# Selection rule (=1 of uniform random [0,1] is lower, so those with higher ps.true more likely to be selected)
selection <- runif(nrow(char)) <= char$ps.select
char.censored <- char[selection,] #remove observations via propensity score rule
# Extract the dependent variable
Y <- char.censored[["out_amountgive"]]
# Extract treatment
W <- char.censored[["treatment"]]
# Extract covariates
covariates <- char.censored[,c(14:22,23:44)]
covariate.names <- names(covariates)
# standardize
covariates.scaled <- scale(covariates)
processed.unscaled <- data.frame(Y, W, covariates)
processed.scaled <- data.frame(Y, W, covariates.scaled)
# training, validation, and test sets.
set.seed(44)
smplmain <- sample(nrow(processed.scaled), round(9*nrow(processed.scaled)/10), replace=FALSE)
processed.scaled.train <- processed.scaled[smplmain,]
processed.scaled.test <- processed.scaled[-smplmain,]
y.train <- as.matrix(processed.scaled.train$Y, ncol=1)
y.test <- as.matrix(processed.scaled.test$Y, ncol=1)
# 45-45-10 sample
smplcausal <- sample(nrow(processed.scaled.train),
round(5*nrow(processed.scaled.train)/10), replace=FALSE)
processed.scaled.train.1 <- processed.scaled.train[smplcausal,]
processed.scaled.train.2 <- processed.scaled.train[-smplcausal,]
# as formulas
print(covariate.names)
sumx = paste(covariate.names, collapse = " + ")  # "X1 + X2 + X3 + ..." for substitution later
interx = paste(" (",sumx, ")^2", sep="")  # "(X1 + X2 + X3 + ...)^2" for substitution later
# Y ~ X1 + X2 + X3 + ...
linearnotreat <- paste("Y",sumx, sep=" ~ ")
linearnotreat <- as.formula(linearnotreat)
linearnotreat
# Y ~ W + X1 + X2 + X3 + ...
linear <- paste("Y",paste("W",sumx, sep=" + "), sep=" ~ ")
linear <- as.formula(linear)
linear
# Y ~ W * (X1 + X2 + X3 + ...)
# ---> X*Z means include these variables plus the interactions between them
linearhet <- paste("Y", paste("W * (", sumx, ") ", sep=""), sep=" ~ ")
linearhet <- as.formula(linearhet)
linearhet
#### LOOK AT THIS AGAIN! (note that this is not used in part I))
##### NEED TO ADJUST PROPENSITY SCORE!
#processed.scaled.test$propens <- mean(processed.scaled.test$W) #note this is randomized experiment so will use constant propens
#processed.scaled.test$Ystar <- processed.scaled.test$W * (processed.scaled.test$Y/processed.scaled.test$propens) -
#  (1-processed.scaled.test$W) * (processed.scaled.test$Y/(1-processed.scaled.test$propens))
#MSElabelvec <- c("")
#MSEvec <- c("")
# causal tree/forest params
# Set parameters
split.Rule.temp = "CT"
cv.option.temp = "CT"
split.Honest.temp = T
cv.Honest.temp = T
split.alpha.temp = .5
cv.alpha.temp = .5
split.Bucket.temp = T
bucketMax.temp= 100
bucketNum.temp = 5
minsize.temp=50
# Some of the models need to get causal effects out by comparing mu(X,W=1)-mu(X,W=0).  Create datasets to do that easily
processed.scaled.testW0 <- processed.scaled.test
processed.scaled.testW0$W <- rep(0,nrow(processed.scaled.test))
processed.scaled.testW1 <- processed.scaled.test
processed.scaled.testW1$W <- rep(1,nrow(processed.scaled.test))
# make this bigger -- say 2000 -- if run time permits
numtreesCT <- 200
numtreesGF <- 200
# Propensity forest
ncolx<-length(processed.scaled.train)-2 #total number of covariates
ncov_sample<-floor(ncolx/3) #number of covariates (randomly sampled) to use to build tree
pf <- propensityForest(as.formula(paste("Y~",sumx)),
data=processed.scaled.train,
treatment=processed.scaled.train$W,
split.Bucket=F,
sample.size.total = floor(nrow(processed.scaled.train) / 2),
nodesize = 25, num.trees=numtreesCT,
mtry=ncov_sample, ncolx=ncolx, ncov_sample=ncov_sample )
pfpredtest <- predict(pf, newdata=processed.scaled.test, type="vector")
pfpredtrainall <- predict(pf, newdata=processed.scaled.train,
predict.all = TRUE, type="vector")
print(c("mean of ATE treatment effect from propensityForest on Training data",
round(mean(pfpredtrainall$aggregate),5)))
pfvar <- infJack(pfpredtrainall$individual, pf$inbag, calibrate = TRUE)
plot(pfvar)
# calculate MSE against Ystar
#pfMSEstar <- mean((processed.scaled.test$Ystar-pfpredtest)^2)
#print(c("MSE using ystar on test set of causalTree/propforest",pfMSEstar))
#MSElabelvec <- append(MSElabelvec,"propensity forest")
#MSEvec <- append(MSEvec,pfMSEstar)
X = as.matrix(processed.scaled.train[,covariate.names])
X.test = as.matrix(processed.scaled.test[,covariate.names])
Y  = as.matrix(processed.scaled.train[,"Y"])
W  = as.matrix(processed.scaled.train[,"W"])
gf <- causal.forest(X, Y, W, num.trees = numtreesGF, ci.group.size = 4,
precompute.nuisance = FALSE)
preds.causal.oob = predict(gf, estimate.variance=TRUE)
preds.causal.test = predict(gf, X.test, estimate.variance=TRUE)
mean(preds.causal.oob$predictions)
plot(preds.causal.oob$predictions, preds.causal.oob$variance.estimates)
mean(preds.causal.test$predictions)
plot(preds.causal.test$predictions, preds.causal.test$variance.estimates)
# calculate MSE against Ystar
#gfMSEstar <- mean((processed.scaled.test$Ystar-preds.causal.test$predictions)^2)
#print(c("MSE using ystar on test set of gradient causal forest",gfMSEstar))
#MSElabelvec <- append(MSElabelvec,"gradient causal forest")
#MSEvec <- append(MSEvec,gfMSEstar)
Yrf <- regression.forest(X, Y, num.trees = numtreesGF, ci.group.size = 4)
Yresid <- Y - predict(Yrf)$prediction
# orthogonalize W -- if obs study
Wrf <- regression.forest(X, W, num.trees = numtreesGF, ci.group.size = 4)
Wresid <- W - predict(Wrf)$predictions # use if you are orthogonalizing W, e.g. for obs study
gfr <- causal.forest(X,Yresid,Wresid,num.trees=numtreesGF, ci.group.size=4,
precompute.nuisance = FALSE)
preds.causalr.oob = predict(gfr, estimate.variance=TRUE)
mean(preds.causalr.oob$predictions)
plot(preds.causalr.oob$predictions, preds.causal.oob$variance.estimates)
Xtest = as.matrix(processed.scaled.test[,covariate.names])
preds.causalr.test = predict(gfr, Xtest, estimate.variance=TRUE)
mean(preds.causalr.test$predictions)
plot(preds.causalr.test$predictions, preds.causal.test$variance.estimates)
# calculate MSE against Ystar
#gfrMSEstar <- mean((processed.scaled.test$Ystar-preds.causalr.test$predictions)^2)
#rint(c("MSE using ystar on test set of orth causal gradient forest",gfrMSEstar))
#MSElabelvec <- append(MSElabelvec,"orth causal gradient forest")
#MSEvec <- append(MSEvec,gfrMSEstar)
