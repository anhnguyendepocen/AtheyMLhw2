knitr::opts_chunk$set(echo = TRUE)
```
## Setup
```{r load, echo=F, message=FALSE, warning=FALSE}
# set your working directory
#setwd("C:/Users/Jack/Documents/Git/AtheyMLhw2") # Jack
setwd('/home/luis/AtheyMLhw2') #Luis
# clear things in RStudio
rm(list = ls())
# Call packages
library(ggplot2)
#library(dplyr)
#library(reshape2)
#library(glmnet)
#library(plotmo)
#library(pogs)
#library(balanceHD)
library(causalTree)
library(randomForestCI)
library(reshape2)
library(plyr)
library(gradient.forest)
library(glmnet)
# set seed
set.seed(1992)
fname <- 'Data/charitable_withdummyvariables.csv'
char <- read.csv(fname)
```
Setup our data ready to feed into functions
```{r obs setup, echo=F, message=FALSE, warning=FALSE}
# Extract the dependent variable
Y <- char[["out_amountgive"]]
# Extract treatment
W <- char[["treatment"]]
# Extract covariates
covariates <- char[,c(14:22,23:63)] # include missing dummies for now
covariate.names <- names(covariates)
# standardize
covariates.scaled <- scale(covariates)
processed.unscaled <- data.frame(Y, W, covariates)
processed.scaled <- data.frame(Y, W, covariates.scaled)
# training, validation, and test sets.
#split evenly into 1/3,1/3,1/3 samples
set.seed(46)
sample.main <- sample(nrow(processed.scaled), round(2*nrow(processed.scaled)/3), replace=FALSE)
processed.scaled.train <- processed.scaled[sample.main,]
processed.scaled.test.C <- processed.scaled[-sample.main,]
y.train <- as.matrix(processed.scaled.train$Y, ncol=1)
y.test.C <- as.matrix(processed.scaled.test.C$Y, ncol=1)
# equal sized sets
sample.causal <- sample(nrow(processed.scaled.train),
round(nrow(processed.scaled.train)/2), replace=FALSE)
processed.scaled.train.A <- processed.scaled.train[sample.causal,]
processed.scaled.train.B <- processed.scaled.train[-sample.causal,]
y.train.1 <- y.train[sample.causal]
y.train.2 <- y.train[-sample.causal]
# combination of B and C (stuff we don't directly train on)
processed.scaled.test.B.C <- rbind(processed.scaled.train.B,processed.scaled.test.C)
# write out RHS formulas for lasso heterogeneity
print(covariate.names)
sumx = paste(covariate.names, collapse = " + ")  # "X1 + X2 + X3 + ..." for substitution later
interx = paste(" (",sumx, ")^2", sep="")  # "(X1 + X2 + X3 + ...)^2" for substitution later
# Y ~ X1 + X2 + X3 + ...
linearnotreat <- paste("Y",sumx, sep=" ~ ")
linearnotreat <- as.formula(linearnotreat)
linearnotreat
# Y ~ W + X1 + X2 + X3 + ...
linear <- paste("Y",paste("W",sumx, sep=" + "), sep=" ~ ")
linear <- as.formula(linear)
linear
# Y ~ W * (X1 + X2 + X3 + ...)
# ---> X*Z means include these variables plus the interactions between them
linearhet <- paste("Y", paste("W * (", sumx, ") ", sep=""), sep=" ~ ")
linearhet <- as.formula(linearhet)
linearhet
#calculate unconditional prop. of treatment within each sample.
processed.scaled.test.C$propens <- mean(processed.scaled.test.C$W)
processed.scaled.test.B.C$propens <- mean(processed.scaled.test.B.C$W)
# Randomized experiment. Use constant propensity score.
# Build Ystar whose Expectation=ATE to evaluate methods against
processed.scaled.test.C$Ystar <- processed.scaled.test.C$W * (processed.scaled.test.C$Y/processed.scaled.test.C$propens) -
(1-processed.scaled.test.C$W) * (processed.scaled.test.C$Y/(1-processed.scaled.test.C$propens))
## Also do this for B.C union
processed.scaled.test.B.C$Ystar <- processed.scaled.test.B.C$W * (processed.scaled.test.B.C$Y/processed.scaled.test.B.C$propens) -
(1-processed.scaled.test.B.C$W) * (processed.scaled.test.B.C$Y/(1-processed.scaled.test.B.C$propens))
# articificially assign B/C samples to treat/control for  ATE calculation later
processed.scaled.test.CW0 <- processed.scaled.test.C
processed.scaled.test.CW0$W <- rep(0,nrow(processed.scaled.test.C))
processed.scaled.test.CW1 <- processed.scaled.test.C
processed.scaled.test.CW1$W <- rep(1,nrow(processed.scaled.test.C))
processed.scaled.test.B.CW0 <- processed.scaled.test.B.C
processed.scaled.test.B.CW0$W <- rep(0,nrow(processed.scaled.test.B.C))
processed.scaled.test.B.CW1 <- processed.scaled.test.B.C
processed.scaled.test.B.CW1$W <- rep(1,nrow(processed.scaled.test.B.C))
# Set up MSE vector to record performance
MSElabelvec <- c("")
MSEvec <- c("")
knitr::opts_chunk$set(echo = TRUE)
# Set parameters
split.Rule.temp = "CT"
cv.option.temp = "CT"
split.Honest.temp = T
cv.Honest.temp = T
split.alpha.temp = .5
cv.alpha.temp = .5
split.Bucket.temp = T
bucketMax.temp= 100
bucketNum.temp = 5
minsize.temp=50
# number of trees (try 1000 once all working)
numtreesCT <- 500
numtreesGF <- 500
# Set parameters
split.Rule.temp = "CT"
cv.option.temp = "CT"
split.Honest.temp = T
cv.Honest.temp = T
split.alpha.temp = .5
cv.alpha.temp = .5
split.Bucket.temp = T
bucketMax.temp= 100
bucketNum.temp = 5
minsize.temp=50
# number of trees (try 1000 once all working)
numtreesCT <- 500
numtreesGF <- 500
# setup models
#create feature matrix for lasso W-int regression
#linear.train <- model.matrix(linearhet, processed.scaled.train)[,-1]
linear.test <- model.matrix(linearhet, processed.scaled.test.C)[,-1]
linear.train.1 <- model.matrix(linearhet, processed.scaled.train.A)[,-1]
linear.train.2 <- model.matrix(linearhet, processed.scaled.train.B)[,-1]
# set lower penalty factor for coef on perbush lower + interaction w/ W
p.fac = rep(1, 1 + 2*ncol(covariates))
names(p.fac) <- colnames(linear.train.1)
p.fac['W']=0
#p.fac['hpa']=0.5
#p.fac['W:hpa'] = 0.5
#p.fac['perbush']=0.5
#p.fac['W:perbush']=0.5
# Fit lasso on sample A w/ L1 penalty
set.seed(1992)
lasso.linear <- cv.glmnet(linear.train.1, y.train.1,  alpha=1, parallel=TRUE,
nfolds=10,penalty.factor = p.fac)
#lasso.linear
# List non-zero coefficients
opt.lasso.coefs<-coef(lasso.linear,s='lambda.min')
selected.vars<-rownames(opt.lasso.coefs)[as.logical(opt.lasso.coefs!=0)][-1]
print(selected.vars)
# plot CV performance over complexity parameter
plot(lasso.linear)
lasso.linear$lambda.min # min average CV error
lasso.linear$lambda.1se #  error within one s.e. of min
# set up as formula for post-selection OLS
#postOLS <- paste("Y", paste(append(selected.vars, "W"),collapse=" + "), sep = " ~ ")
postOLS <- paste("Y ~",paste(selected.vars,collapse="+"))
postOLS <- as.formula(postOLS)
# OLS using these coefficients on sample A
lm.linear.lasso.A <- lm(postOLS, data=processed.scaled.train.A)
summary(lm.linear.lasso.A)
# OLS using these coefficients on sample B
lm.linear.lasso.B <- lm(postOLS, data=processed.scaled.train.B)
summary(lm.linear.lasso.B)
# OLS using these coefficients on sample C
lm.linear.lasso.C <- lm(postOLS, data=processed.scaled.test.C)
summary(lm.linear.lasso.C)
# OLS using union of samples B and C
lm.linear.lasso.B.C <- lm(postOLS, data=processed.scaled.test.B.C)
summary(lm.linear.lasso.B.C)
##############
# Predict Y on union of B and C from postOLS regression, under two counterfactual treatments, for average treatment effect
predictedW0 <- predict(lm.linear.lasso.B.C, newdata=processed.scaled.test.B.CW0)
predictedW1 <- predict(lm.linear.lasso.B.C, newdata=processed.scaled.test.B.CW1)
lassocauseff <- predictedW1-predictedW0
print(mean(lassocauseff))
# calculate MSE against Ystar (whose expectation is CATE)
lassoMSEstar <- mean((processed.scaled.test.B.C$Ystar-lassocauseff)^2)
print(c("MSE using ystar on test set of lasso",lassoMSEstar))
MSElabelvec <- append(MSElabelvec,"lasso")
MSEvec <- append(MSEvec,lassoMSEstar)
split.Rule.temp = "CT"
cv.option.temp = "CT"
split.Honest.temp = T
cv.Honest.temp = T
#do regular honest weighting
split.alpha.temp = .5
cv.alpha.temp = .5
#don't do bucket splitting for now
split.Bucket.temp = T
bucketMax.temp= 100
bucketNum.temp = 5
minsize.temp=50
# fit on A, estimate on B
CTtree <- honest.causalTree(as.formula(paste("Y~",sumx)),
data=processed.scaled.train.A, treatment=processed.scaled.train.A$W,
est_data = processed.scaled.train.B, est_treatment = processed.scaled.train.B$W,
split.Rule=split.Rule.temp,split.Honest=cv.Honest=cv.Honest.temp,
split.Rule.temp = "CT"
cv.option.temp = "CT"
split.Honest.temp = T
cv.Honest.temp = T
#do regular honest weighting
split.alpha.temp = .5
cv.alpha.temp = .5
#do bucket splitting for now
split.Bucket.temp = T
bucketMax.temp= 100
bucketNum.temp = 5
minsize.temp=50
# fit on A, estimate on B
CTtree <- honest.causalTree(as.formula(paste("Y~",sumx)),
data=processed.scaled.train.A, treatment=processed.scaled.train.A$W,
est_data = processed.scaled.train.B, est_treatment = processed.scaled.train.B$W,
split.Rule=split.Rule.temp,split.Honest=split.Honest.temp,
split.Bucket=split.Bucket.temp, bucketNum = bucketNum.temp, bucketMax = bucketMax.temp,
cv.option=cv.option.temp, cv.Honest=cv.Honest.temp,
minsize = minsize.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
HonestSampleSize=nrow(processed.scaled.train.B))
CTtree$cp
opcpid <- which.min(CTtree$cp[,4])
opcid
opcpid
opcp <- CTtree$cp[opcpid,1]
tree_honest_CT_prune <- prune(CTtree, cp = opcp)
print(tree_honest_CT_prune)
plot(tree_honest_CT_prune)
prp(tree_honest_CT_prune)
# create leaf IDs for samples B,C up to 4 decimal places
processed.scaled.train.B$leaffact <- as.factor(round(predict(tree_honest_CT_prune,
newdata=processed.scaled.train.B,type="vector"),4))
processed.scaled.train.C$leaffact <- as.factor(round(predict(tree_honest_CT_prune,
newdata=processed.scaled.train.C,type="vector"),4))
# create leaf IDs for samples A,B,C up to 4 decimal places
processed.scaled.train.A$leaffact <- as.factor(round(predict(tree_honest_CT_prune,
newdata=processed.scaled.train.A,type="vector"),4))
processed.scaled.train.B$leaffact <- as.factor(round(predict(tree_honest_CT_prune,
newdata=processed.scaled.train.B,type="vector"),4))
processed.scaled.train.C$leaffact <- as.factor(round(predict(tree_honest_CT_prune,
newdata=processed.scaled.test.C,type="vector"),4))
processed.scaled.test.C$leaffact <- as.factor(round(predict(tree_honest_CT_prune,
newdata=processed.scaled.test.C,type="vector"),4))
# These show leaf treatment effects and standard errors; can test hypothesis that leaf
# treatment effects are 0
summary(lm(Y~leaffact+W*leaffact-W-1, data=processed.scaled.train.A))
summary(lm(Y~leaffact+W*leaffact-W-1, data=processed.scaled.train.B))
summary(lm(Y~leaffact+W*leaffact-W-1, data=processed.scaled.test.C))
# Test whether each leaf treatment effects are different than ATE (uninteracted W)
summary(lm(Y~leaffact+W*leaffact-1, data=processed.scaled.test.C))
# Predict using C (test data)
CTpredict = predict(tree_honest_CT_prune, newdata=processed.scaled.test.C, type="vector")
# calculate MSE against Ystar
# calculate MSE against Ystar
CTMSEstar <- mean((processed.scaled.test.C$Ystar-CTpredict)^2)
print(c("MSE using ystar on test set of single tree",CTMSEstar))
MSElabelvec <- append(MSElabelvec,"causal tree")
MSEvec <- append(MSEvec,CTMSEstar)
MSEvec
?causalForest
#no need to split independently A/B since sthe forest routine does this honest splitting for us
cf <- causalForest(as.formula(paste("Y~",sumx)), data=processed.scaled.train,
treatment=processed.scaled.train$W,  double.Sample = T,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
split.Bucket=split.Bucket.temp,
bucketNum = bucketNum.temp,
bucketMax = bucketMax.temp, cv.option=cv.option.temp,
cv.Honest=cv.Honest.temp, minsize = minsize.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
sample.size.total = floor(nrow(processed.scaled.train) / 2),
sample.size.train.frac = .5,
nodesize = minsize.temp, num.trees= numtreesCT,
ncolx=ncolx,ncov_sample=ncov_sample)
ncolx<-length(processed.scaled.train)-3 # number of possible covariates to bag from
ncov_sample<-floor(2*ncolx/3) #number of covariates (randomly sampled) to use to build tree
# ncov_sample<-p #use this line if all covariates need to be used in all trees
# set seed
set.seed(123)
# now estimate a causalForest. Train on A and B.
#no need to split independently A/B since sthe forest routine does this honest splitting for us
cf <- causalForest(as.formula(paste("Y~",sumx)), data=processed.scaled.train,
treatment=processed.scaled.train$W,  double.Sample = T,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
split.Bucket=split.Bucket.temp,
bucketNum = bucketNum.temp,
bucketMax = bucketMax.temp, cv.option=cv.option.temp,
cv.Honest=cv.Honest.temp, minsize = minsize.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
sample.size.total = floor(nrow(processed.scaled.train) / 2),
sample.size.train.frac = .5,
nodesize = minsize.temp, num.trees= numtreesCT,
ncolx=ncolx,ncov_sample=ncov_sample)
# number of trees (try 1000 once all working)
numtreesCT <- 1000
numtreesGF <- 1000
set.seed(123)
# now estimate a causalForest. Train on A and B.
#no need to split independently A/B since sthe forest routine does this honest splitting for us
cf <- causalForest(as.formula(paste("Y~",sumx)), data=processed.scaled.train,
treatment=processed.scaled.train$W,  double.Sample = T,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
split.Bucket=split.Bucket.temp,
bucketNum = bucketNum.temp,
bucketMax = bucketMax.temp, cv.option=cv.option.temp,
cv.Honest=cv.Honest.temp, minsize = minsize.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
sample.size.total = floor(nrow(processed.scaled.train) / 2),
sample.size.train.frac = .5,
nodesize = minsize.temp, num.trees= numtreesCT,
ncolx=ncolx,ncov_sample=ncov_sample)
cfpredtest <- predict(cf, newdata=processed.scaled.test.C, type="vector")
print(cfpredtest)
summary(cfpredtest)
unique(cfpredtest)
cfpredtrainall <- predict(cf, newdata=processed.scaled.train,
predict.all = TRUE, type="vector")
unique(cfpredtrainall)
mean(cfpredtrainall)
summary(cfpredtrainall$aggregate)
unique(cfpredtrainall$aggregate)
unique(cfpredtrainall$individual)
summary(cfpredtrainall$individual)
dim(cfpredtrainall$individual)
dim(cfpredtrainall$aggregate)
length(cfpredtrainall$aggregate)
cfpredtrainall$individual[1,]
mean(cfpredtrainall$individual[1,])
mean(cfpredtrainall$individual[2,])
mean(cfpredtrainall$individual[3,])
mean(cfpredtrainall$individual[2000,])
mean(cfpredtrainall$individual[2000,])==0
mean(cfpredtrainall$individual[2000,]==cfpredtrainall$individual[2,])
mean(cfpredtrainall$individual[2000,]==cfpredtrainall$individual[6,])
mean(cfpredtrainall$individual[2000,]==cfpredtrainall$individual[2000,])
mean(cfpredtrainall$individual[2000,]==cfpredtrainall$individual[1000,])
unique(cfpredtrainall$individual)
?rm
rm(cf)
cf <- causalForest(as.formula(paste("Y~",sumx)), data=processed.scaled.train,
treatment=processed.scaled.train$W,  double.Sample = T,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
split.Bucket=F,
bucketNum = bucketNum.temp,
bucketMax = bucketMax.temp, cv.option=cv.option.temp,
cv.Honest=cv.Honest.temp, minsize = minsize.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
sample.size.total = floor(nrow(processed.scaled.train) / 2),
sample.size.train.frac = .5,
nodesize = minsize.temp, num.trees= numtreesCT,
ncolx=ncolx,ncov_sample=ncov_sample)
summary(cf)
summary(cf$trees)
cf$inbag
summary(colmeans(cf$inbag))
summary(colMeans(cf$inbag))
summary(rowMeans(cf$inbag))
dim(cf$inbag)
cfpredtest <- predict(cf, newdata=processed.scaled.test.C, type="vector")
summary(cfpredtest)
summary(cfpredtest)
summary(cf)
zzzz<- cf$cov_sample
zzzz[1,]
zzzz[2,]
ncov_sample
zz<- cf$fsample
zz[1]
zz[2]
zz[3]
cfpredtrainall <- predict(cf, newdata=processed.scaled.train,
predict.all = TRUE, type="vector")
summary(cfpredtrainall$aggregate)
hist(cfpredtrainall$aggregate)
print(c("mean of ATE treatment effect from causalForest on Training data",
round(mean(cfpredtrainall$aggregate),5)))
print(c("mean of ATE treatment effect from causalForest on Test data",
round(mean(cfpredtest),5)))
processed.scaled.train['Y']
summary(processed.scaled.train['Y'])
ncolx
ncolx<-length(processed.scaled.train)-3 # number of possible covariates to bag from
ncov_sample<-floor(ncolx/3) #number of covariates (randomly sampled) to use to build tree
# ncov_sample<-p #use this line if all covariates need to be used in all trees
# set seed
set.seed(123)
# now estimate a causalForest. Train on A and B.
#no need to split independently A/B since the forest routine does this honest splitting for us
rm(cf)
cf <- causalForest(as.formula(paste("Y~",sumx)), data=processed.scaled.train,
treatment=processed.scaled.train$W,  double.Sample = T,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
split.Bucket=F,
bucketNum = bucketNum.temp,
bucketMax = bucketMax.temp, cv.option=cv.option.temp,
cv.Honest=cv.Honest.temp, minsize = minsize.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
sample.size.total = floor(nrow(processed.scaled.train) / 2),
sample.size.train.frac = .5,
nodesize = 5, num.trees= numtreesCT,
ncolx=ncolx,ncov_sample=ncov_sample)
cfpredtrainall <- predict(cf, newdata=processed.scaled.train,
predict.all = TRUE, type="vector")
summary(cfpredtrainall$individual)
summary(c(cfpredtrainall$individual))
set.seed(123)
# now estimate a causalForest. Train on A and B.
#no need to split independently A/B since the forest routine does this honest splitting for us
rm(cf)
cf <- causalForest(as.formula(paste("Y~",sumx)), data=processed.scaled.train,
treatment=processed.scaled.train$W,  double.Sample = T,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
split.Bucket=F,
bucketNum = bucketNum.temp,
bucketMax = bucketMax.temp, cv.option=cv.option.temp,
cv.Honest=cv.Honest.temp, minsize = 5,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
sample.size.total = floor(nrow(processed.scaled.train) / 2),
sample.size.train.frac = .5,
nodesize = 5, num.trees= 100,
ncolx=ncolx,ncov_sample=ncov_sample)
cfpredtrainall <- predict(cf, newdata=processed.scaled.train,
predict.all = TRUE, type="vector")
summary(c(cfpredtrainall$individual))
tt <- cf$trees[1]
tt
tt <- cf$trees[50]
tt
?causalForest
formula
cf.formula<-as.formula(paste("Y~",sumx))
cf.formula
sum(processed.scaled.train$W)
length(processed.scaled.train$W)
#no need to split independently A/B since the forest routine does this honest splitting for us
rm(cf)
cf.formula<-as.formula(paste("Y~",sumx))
cf <- causalForest(cf.formula, data=processed.scaled.train,
treatment=processed.scaled.train$W,  double.Sample = T,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
split.Bucket=F,
bucketNum = bucketNum.temp,
bucketMax = bucketMax.temp, cv.option=cv.option.temp,
cv.Honest=cv.Honest.temp, minsize = 5,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
sample.size.total = floor(nrow(processed.scaled.train)*.8),
sample.size.train.frac = .8,
nodesize = 5, num.trees= 100,
ncolx=ncolx,ncov_sample=ncov_sample)
# now estimate a causalForest. Train on A and B.
#no need to split independently A/B since the forest routine does this honest splitting for us
rm(cf)
cf.formula<-as.formula(paste("Y~",sumx))
cf <- causalForest(cf.formula, data=processed.scaled.train,
treatment=processed.scaled.train$W,  double.Sample = T,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
split.Bucket=F,
bucketNum = bucketNum.temp,
bucketMax = bucketMax.temp, cv.option=cv.option.temp,
cv.Honest=cv.Honest.temp, minsize = 5,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
sample.size.total = floor(nrow(processed.scaled.train)*.8),
sample.size.train.frac = .8,
num.trees= 100,
ncolx=ncolx,ncov_sample=ncov_sample)
cfpredtrainall <- predict(cf, newdata=processed.scaled.train,
predict.all = TRUE, type="vector")
summary(c(cfpredtrainall$individual))
ttt<- cf$trees[1]
ttt
attributes(ttt)
ttt[1]
ttt[2]
ttt[1]
ttt[1][1]
length(tt)
length(ttt)
rm(cf)
cf.formula<-as.formula(paste("Y~",sumx))
cf <- causalForest(cf.formula, data=processed.scaled.train.A,
treatment=processed.scaled.train.A$W,  double.Sample = T,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
split.Bucket=F,
bucketNum = bucketNum.temp,
bucketMax = bucketMax.temp, cv.option=cv.option.temp,
cv.Honest=cv.Honest.temp, minsize = 5,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
sample.size.total = floor(nrow(processed.scaled.train)*.5),
sample.size.train.frac = .5,
num.trees= 10,
ncolx=ncolx,ncov_sample=ncov_sample)
cfpredtrainall <- predict(cf, newdata=processed.scaled.train,
predict.all = TRUE, type="vector")
summary(c(cfpredtrainall$individual))
install_github("susanathey/causalTree", ref="master", force=TRUE)
library(devtools)
install_github("susanathey/causalTree", ref="master", force=TRUE)
rm(cf)
cf.formula<-as.formula(paste("Y~",sumx))
cf <- causalForest(cf.formula, data=processed.scaled.train.A,
treatment=processed.scaled.train.A$W,  double.Sample = T,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
split.Bucket=F,
bucketNum = bucketNum.temp,
bucketMax = bucketMax.temp, cv.option=cv.option.temp,
cv.Honest=cv.Honest.temp, minsize = 5,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
sample.size.total = floor(nrow(processed.scaled.train)*.5),
sample.size.train.frac = .5,
num.trees= 10,
ncolx=ncolx,ncov_sample=ncov_sample)
cfpredtest <- predict(cf, newdata=processed.scaled.test.C, type="vector")
summary(c(cfpredtest))
cfpredtrain.A <- predict(cf, predict.all = TRUE, type="vector")
