cf$trees[2]
set.seed(123)
cf.formula<-as.formula(paste("Y~",sumx))
cf <- causalForest(cf.formula, data=processed.scaled.train.A,
treatment=processed.scaled.train.A$W,num.trees=10,
double.Sample = T, split.Bucket=F,
sample.size.total = floor(nrow(processed.scaled.train.A)*.2)
sample.size.train.frac = .5,minsize = 5,
cv.option=cv.option.temp, cv.Honest=cv.Honest.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
ncolx = ncolx,ncov_sample=ncov_sample)
set.seed(123)
cf.formula<-as.formula(paste("Y~",sumx))
cf <- causalForest(cf.formula, data=processed.scaled.train.A,
treatment=processed.scaled.train.A$W,num.trees=10,
double.Sample = T, split.Bucket=F,
sample.size.total = floor(nrow(processed.scaled.train.A)*.2),
sample.size.train.frac = .5,minsize = 5,
cv.option=cv.option.temp, cv.Honest=cv.Honest.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
ncolx = ncolx,ncov_sample=ncov_sample)
cf$trees[1]
cf$trees[2]
cf$trees[3]
cf$trees[5]
cf$trees[67]
cf$trees[7]
cf$trees[6]
cf$trees[5]
cf$trees[4]
cf$trees[3]
set.seed(123)
cf.formula<-as.formula(paste("Y~",sumx))
cf <- causalForest(cf.formula, data=processed.scaled.train.A,
treatment=processed.scaled.train.A$W,num.trees=10,
double.Sample = T, split.Bucket=F,
sample.size.total = floor(nrow(processed.scaled.train.A)*.1),
sample.size.train.frac = .5,minsize = 5,
cv.option=cv.option.temp, cv.Honest=cv.Honest.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
ncolx = ncolx,ncov_sample=ncov_sample)
cf$trees[3]
set.seed(123)
cf.formula<-as.formula(paste("Y~",sumx))
cf <- causalForest(cf.formula, data=processed.scaled.train,
treatment=processed.scaled.train$W,num.trees=10,
double.Sample = T, split.Bucket=F,
sample.size.total = floor(nrow(processed.scaled.train.A)*.1),
sample.size.train.frac = .5,minsize = 5,
cv.option=cv.option.temp, cv.Honest=cv.Honest.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
ncolx = ncolx,ncov_sample=ncov_sample)
cf$trees[3]
cf$trees[1]
cf$trees[2]
cf$trees[54]
cf$trees[4]
cf$trees[5]
cf$trees[6]
cf$trees[7]
cf$trees[4]
cf$trees[5]
cf$trees[6]
rpart.plot(cf$trees[6])
rpart.plot(cf$trees[6][[1]])
set.seed(123)
cf.formula<-as.formula(paste("Y~",sumx))
cf <- causalForest(cf.formula, data=processed.scaled.train,
treatment=processed.scaled.train$W,num.trees=10,
double.Sample = T, split.Bucket=F,
sample.size.total = floor(nrow(processed.scaled.train)*.1),
sample.size.train.frac = .5,minsize = 5,
cv.option=cv.option.temp, cv.Honest=cv.Honest.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
ncolx = ncolx,ncov_sample=ncov_sample)
cf$trees[6]
cf$trees[3]
cf$trees[2]
cf$trees[5]
cf$trees[1]
cf$trees[2]
cf$trees[7]
cf$trees[8]
cf$trees[9]
set.seed(123)
cf.formula<-as.formula(paste("Y~",sumx))
cf <- causalForest(cf.formula, data=processed.scaled.train,
treatment=processed.scaled.train$W,num.trees=10,
double.Sample = T, split.Bucket=F,
sample.size.total = floor(nrow(processed.scaled.train)/20),
sample.size.train.frac = .5,minsize = 5,
cv.option=cv.option.temp, cv.Honest=cv.Honest.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
ncolx = ncolx,ncov_sample=ncov_sample)
cf$trees[9]
# set seed
set.seed(123)
# now estimate a causalForest. Train on A and B.
#no need to split independently A/B since the forest routine does this honest splitting for us
#rm(cf)
cf.formula<-as.formula(paste("Y~",sumx))
cf <- causalForest(cf.formula, data=processed.scaled.train,
treatment=processed.scaled.train$W,num.trees=1000,
double.Sample = T, split.Bucket=F,
sample.size.total = floor(nrow(processed.scaled.train)/20),
sample.size.train.frac = .5,minsize = 5,
cv.option=cv.option.temp, cv.Honest=cv.Honest.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
ncolx = ncolx,ncov_sample=ncov_sample)
cfpredtest <- predict(cf, newdata=processed.scaled.test.C, type="vector")
cfpredtrainall <- predict(cf, newdata=processed.scaled.train.A,
predict.all = TRUE, type="vector")
summary(c(cfpredtrainall$individual))
summary(cfpredtrainall$aggregate)
# MSE against Ystar
cfMSEstar <- mean((processed.scaled.test.C$Ystar-cfpredtest)^2)
print(c("MSE using ystar on test set of causalTree/causalForest",cfMSEstar))
mean(cfMSEstar)
# ATE
print(c("mean of ATE treatment effect from causalForest on Training data",
round(mean(cfpredtrainall$aggregate),5)))
print(c("mean of ATE treatment effect from causalForest on Test data",
round(mean(cfpredtest),5)))
# use infJack routine from randomForestCI
# This gives variances for each of the estimated treatment effects; note tau is labelled y.hat
cfvar <- infJack(cfpredtrainall$individual, cf$inbag, calibrate = TRUE)
plot(cfvar)
?causalForest
cf.formula<-as.formula(paste("Y~",sumx))
cf <- causalForest(cf.formula, data=processed.scaled.train,
treatment=processed.scaled.train$W,num.trees=1000,
double.Sample = T, split.Bucket=F,
sample.size.total = floor(nrow(processed.scaled.train)/20),
sample.size.train.frac = .5,minsize = 5,
cv.option=cv.option.temp, cv.Honest=cv.Honest.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
ncolx = ncolx,ncov_sample=ncov_sample,mtry=ncov_sample)
# set seed
set.seed(123)
# now estimate a causalForest. Train on A and B.
#no need to split independently A/B since the forest routine does this honest splitting for us
#rm(cf)
cf.formula<-as.formula(paste("Y~",sumx))
cf <- causalForest(cf.formula, data=processed.scaled.train,
treatment=processed.scaled.train$W,num.trees=1000,
double.Sample = T, split.Bucket=F,
sample.size.total = floor(nrow(processed.scaled.train)/20),
sample.size.train.frac = .5,minsize = 5,
cv.option=cv.option.temp, cv.Honest=cv.Honest.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
ncolx = ncolx,ncov_sample=ncov_sample,mtry=ncov_sample)
cfpredtest <- predict(cf, newdata=processed.scaled.test.C, type="vector")
cfpredtrainall <- predict(cf, newdata=processed.scaled.train.A,
predict.all = TRUE, type="vector")
summary(c(cfpredtrainall$aggregate))
summary(cfpredtest)
MSEvec
?invisible
cv.second.AJR <- read.csv('/home/luis/CausalML-project/DeepIV/AJR/cv_mp_output/cv_secondstage.csv')
ggplot(data=cv.second.AJR,aes(x=node,y=mean)) + geom_line()+ geom_ribbon(aes(ymin=mean-se, ymax=mean+se),fill=NA,colour='red',linetype='dashed',alpha=0.3) +
ylab('MSE') + xlab('# of Hidden Layer Nodes')  + ggtitle('Test CV over node choices for second stage +/- CV SE') #+ coord_cartesian(ylim = c(0,3))
cv.second.AJR <- read.csv('/home/luis/CausalML-project/DeepIV/AJR/cv_mp_output/cv_secondstage.csv')
ggplot(data=cv.second.AJR,aes(x=node,y=mean)) + geom_line()+ geom_ribbon(aes(ymin=mean-se, ymax=mean+se),fill=NA,colour='red',linetype='dashed',alpha=0.3) +
ylab('MSE') + xlab('# of Hidden Layer Nodes')  + ggtitle('Test CV over node choices for second stage +/- CV SE') + coord_cartesian(ylim = c(0,3))
cv.second.AJR <- read.csv('/home/luis/CausalML-project/DeepIV/AJR/cv_mp_output/cv_secondstage.csv')
ggplot(data=cv.second.AJR,aes(x=node,y=mean)) + geom_line()+ geom_ribbon(aes(ymin=mean-se, ymax=mean+se),fill=NA,colour='red',linetype='dashed',alpha=0.3) +
ylab('MSE') + xlab('# of Hidden Layer Nodes')  + ggtitle('Test CV over node choices for second stage +/- CV SE') + coord_cartesian(ylim = c(0,3))
ggsave('/home/luis/CausalML-project/DeepIV/AJR/CV_2ndStage_graph.pdf')
log<-capture.output({
cf <- causalForest(cf.formula, data=processed.scaled.train,
treatment=processed.scaled.train$W,num.trees=numtreesCTd,
double.Sample = T, split.Bucket=F,
sample.size.total = floor(nrow(processed.scaled.train)/20),
sample.size.train.frac = .5,minsize = 5,
cv.option=cv.option.temp, cv.Honest=cv.Honest.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
ncolx = ncolx,ncov_sample=ncov_sample,mtry=ncov_sample)
})
log<-capture.output({
cf <- causalForest(cf.formula, data=processed.scaled.train,
treatment=processed.scaled.train$W,num.trees=numtreesCT,
double.Sample = T, split.Bucket=F,
sample.size.total = floor(nrow(processed.scaled.train)/20),
sample.size.train.frac = .5,minsize = 5,
cv.option=cv.option.temp, cv.Honest=cv.Honest.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
ncolx = ncolx,ncov_sample=ncov_sample,mtry=ncov_sample)
})
print(MSElabelvec)
print(MSEvec)
# set your working directory
#setwd("C:/Users/Jack/Documents/Git/AtheyMLhw2") # Jack
setwd('/home/luis/AtheyMLhw2') #Luis
# clear things in RStudio
rm(list = ls())
# Call packages
library(ggplot2)
#library(dplyr)
#library(reshape2)
#library(glmnet)
#library(plotmo)
#library(pogs)
#library(balanceHD)
library(causalTree)
library(randomForestCI)
library(reshape2)
library(plyr)
library(gradient.forest)
library(glmnet)
# set seed
set.seed(1992)
fname <- 'Data/charitable_withdummyvariables.csv'
char <- read.csv(fname)
#replace -999s with 0s (since there are already missing dummies)
for (v in names(char)){
mi_v <- paste(v,'_missing',sep='')
if (mi_v %in% names(char)){
print(paste('fixing',v))
char[(char[,mi_v]==1),v]<-0
}
}
# Extract the dependent variable
Y <- char[["out_amountgive"]]
# Extract treatment
W <- char[["treatment"]]
# Extract covariates
covariates <- char[,c(14:22,23:63)] # include missing dummies for now
covariate.names <- names(covariates)
# standardize
covariates.scaled <- scale(covariates)
processed.unscaled <- data.frame(Y, W, covariates)
processed.scaled <- data.frame(Y, W, covariates.scaled)
# training, validation, and test sets.
#split evenly into 1/3,1/3,1/3 samples
set.seed(46)
sample.main <- sample(nrow(processed.scaled), round(2*nrow(processed.scaled)/3), replace=FALSE)
processed.scaled.train <- processed.scaled[sample.main,]
processed.scaled.test.C <- processed.scaled[-sample.main,]
y.train <- as.matrix(processed.scaled.train$Y, ncol=1)
y.test.C <- as.matrix(processed.scaled.test.C$Y, ncol=1)
# equal sized sets
sample.causal <- sample(nrow(processed.scaled.train),
round(nrow(processed.scaled.train)/2), replace=FALSE)
processed.scaled.train.A <- processed.scaled.train[sample.causal,]
processed.scaled.train.B <- processed.scaled.train[-sample.causal,]
y.train.1 <- y.train[sample.causal]
y.train.2 <- y.train[-sample.causal]
# combination of B and C (stuff we don't directly train on)
processed.scaled.test.B.C <- rbind(processed.scaled.train.B,processed.scaled.test.C)
# write out RHS formulas for lasso heterogeneity
print(covariate.names)
sumx = paste(covariate.names, collapse = " + ")  # "X1 + X2 + X3 + ..." for substitution later
interx = paste(" (",sumx, ")^2", sep="")  # "(X1 + X2 + X3 + ...)^2" for substitution later
# Y ~ X1 + X2 + X3 + ...
linearnotreat <- paste("Y",sumx, sep=" ~ ")
linearnotreat <- as.formula(linearnotreat)
#linearnotreat
# Y ~ W + X1 + X2 + X3 + ...
linear <- paste("Y",paste("W",sumx, sep=" + "), sep=" ~ ")
linear <- as.formula(linear)
#linear
# Y ~ W * (X1 + X2 + X3 + ...)
# ---> X*Z means include these variables plus the interactions between them
linearhet <- paste("Y", paste("W * (", sumx, ") ", sep=""), sep=" ~ ")
linearhet <- as.formula(linearhet)
#linearhet
#calculate unconditional prop. of treatment within each sample.
processed.scaled.test.C$propens <- mean(processed.scaled.test.C$W)
processed.scaled.test.B.C$propens <- mean(processed.scaled.test.B.C$W)
# Randomized experiment. Use constant propensity score.
# Build Ystar whose Expectation=ATE to evaluate methods against
processed.scaled.test.C$Ystar <- processed.scaled.test.C$W * (processed.scaled.test.C$Y/processed.scaled.test.C$propens) -
(1-processed.scaled.test.C$W) * (processed.scaled.test.C$Y/(1-processed.scaled.test.C$propens))
## Also do this for B.C union
processed.scaled.test.B.C$Ystar <- processed.scaled.test.B.C$W * (processed.scaled.test.B.C$Y/processed.scaled.test.B.C$propens) -
(1-processed.scaled.test.B.C$W) * (processed.scaled.test.B.C$Y/(1-processed.scaled.test.B.C$propens))
# articificially assign B/C samples to treat/control for  ATE calculation later
processed.scaled.test.CW0 <- processed.scaled.test.C
processed.scaled.test.CW0$W <- rep(0,nrow(processed.scaled.test.C))
processed.scaled.test.CW1 <- processed.scaled.test.C
processed.scaled.test.CW1$W <- rep(1,nrow(processed.scaled.test.C))
processed.scaled.test.B.CW0 <- processed.scaled.test.B.C
processed.scaled.test.B.CW0$W <- rep(0,nrow(processed.scaled.test.B.C))
processed.scaled.test.B.CW1 <- processed.scaled.test.B.C
processed.scaled.test.B.CW1$W <- rep(1,nrow(processed.scaled.test.B.C))
# Set up MSE vector to record performance
MSElabelvec <- c("")
MSEvec <- c("")
# causal tree/forest params
# Set parameters
split.Rule.temp = "CT"
cv.option.temp = "CT"
split.Honest.temp = T
cv.Honest.temp = T
split.alpha.temp = .5
cv.alpha.temp = .5
split.Bucket.temp = T
bucketMax.temp= 100
bucketNum.temp = 5
minsize.temp=50
# number of trees (try 1000 once all working)
numtreesCT <- 1000
numtreesGF <- 1000
# Set parameters
split.Rule.temp = "CT"
cv.option.temp = "CT"
split.Honest.temp = T
cv.Honest.temp = T
#do regular honest weighting
split.alpha.temp = .5
cv.alpha.temp = .5
#do bucket splitting for now
split.Bucket.temp = T
bucketMax.temp= 100
bucketNum.temp = 10
minsize.temp=50
ncolx<-length(processed.scaled.train)-3 # number of possible covariates to bag from
ncov_sample<-floor(ncolx/3) #number of covariates (randomly sampled) to use to build tree
# ncov_sample<-p #use this line if all covariates need to be used in all trees
CTtree <- honest.causalTree(as.formula(paste("Y~",sumx)),
data=processed.scaled.train.A, treatment=processed.scaled.train.A$W,
est_data = processed.scaled.train.B, est_treatment = processed.scaled.train.B$W,
split.Rule=split.Rule.temp,split.Honest=split.Honest.temp,
split.Bucket=split.Bucket.temp, bucketNum = bucketNum.temp, bucketMax = bucketMax.temp,
cv.option=cv.option.temp, cv.Honest=cv.Honest.temp,
minsize = minsize.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
HonestSampleSize=nrow(processed.scaled.train.B))
#choose the optimal complexity penalty based on cross validation error
opcpid <- which.min(CTtree$cp[,'xerror'])
opcp <- CTtree$cp[opcpid,1]
tree_honest_CT_prune <- prune(CTtree, cp = opcp)
#plot CV-tunes optimal honest tree
print(tree_honest_CT_prune)
CTtree
CTtree$cptable
rpart.plot(tree_honest_CT_prune)
prp(tree_honest_CT_prune)
getwd()
setwd('/home/luis/AtheyMLhw2') #Luis
getwd()
?setwd
setwd('/home/luis/AtheyMLhw2')
getwd()
pdf('./plots/honestCT.pdf')
prp(tree_honest_CT_prune)
dev.off()
set.seed(123)
# now estimate a causalForest. Train on both A and B.
#no need to split independently A/B since the forest routine does this honest splitting for us
#rm(cf)
# fit a CT on A, estimate on B
cf.formula<-as.formula(paste("Y~",sumx))
#suppress output
log<-capture.output({
cf <- causalForest(cf.formula, data=processed.scaled.train,
treatment=processed.scaled.train$W,num.trees=100,
double.Sample = T, split.Bucket=F,
sample.size.total = floor(nrow(processed.scaled.train)/20),
sample.size.train.frac = .5,minsize = 5,
cv.option=cv.option.temp, cv.Honest=cv.Honest.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
ncolx = ncolx,ncov_sample=ncov_sample,mtry=ncov_sample)
})
prp(cf$trees[1])
cf$trees
cf$trees[1]
cf$trees[[1]]
print(cf$trees[[1]])
prp(cf$trees[[1]])
prp(cf$trees[1])
prp(cf$trees[[4]])
prp(cf$trees[[87]])
mean(processed.scaled.train$W)
mean(processed.scaled.train$Y)
summary(processed.scaled.train$Y)
set.seed(123)
# now estimate a causalForest. Train on both A and B.
#no need to split independently A/B since the forest routine does this honest splitting for us
#rm(cf)
# fit a CT on A, estimate on B
cf.formula<-as.formula(paste("Y~",sumx))
#suppress output
log<-capture.output({
cf <- causalForest(cf.formula, data=processed.scaled.train,
treatment=processed.scaled.train$W,num.trees=100,
double.Sample = T, split.Bucket=F,
sample.size.total = floor(nrow(processed.scaled.train)/20),
sample.size.train.frac = .5,minsize = 5,
cv.option=cv.option.temp, cv.Honest=cv.Honest.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
ncolx = ncolx,ncov_sample=ncolx,mtry=ncolx)
})
prp(cf$trees[[87]])
prp(cf$trees[[1]])
prp(cf$trees[[22]])
prp(cf$trees[[22]])
install_github("susanathey/causalTree", ref="master", force=TRUE)
library(devtools)
install_github("susanathey/causalTree", ref="master", force=TRUE)
source(causalForest())
?viewsource
summary(cf)
?honest.causalTree
CTtree <- honest.causalTree(as.formula(paste("Y~",sumx)),
data=processed.scaled.train.A, treatment=processed.scaled.train.A$W,
est_data = processed.scaled.train.B, est_treatment = processed.scaled.train.B$W,
split.Rule=split.Rule.temp,split.Honest=split.Honest.temp,
split.Bucket=split.Bucket.temp, bucketNum = bucketNum.temp, bucketMax = bucketMax.temp,
cv.option=cv.option.temp, cv.Honest=cv.Honest.temp,
minsize = minsize.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
HonestSampleSize=nrow(processed.scaled.train.B),cp=0)
CTtree$cp
?honest.causalTree
sds
?honest.causalTree
opcpid <- which.min(CTtree$cp[,'xerror'])
opcp <- CTtree$cp[opcpid,1]
opcp
?causalTree
?causalTree
CTtree <- honest.causalTree(as.formula(paste("Y~",sumx)),
data=processed.scaled.train.A, treatment=processed.scaled.train.A$W,
est_data = processed.scaled.train.B, est_treatment = processed.scaled.train.B$W,
split.Rule=split.Rule.temp,split.Honest=split.Honest.temp,
split.Bucket=split.Bucket.temp, bucketNum = bucketNum.temp, bucketMax = bucketMax.temp,
cv.option=cv.option.temp, cv.Honest=cv.Honest.temp,
minsize = minsize.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
HonestSampleSize=nrow(processed.scaled.train.B),cp=0.01)
HonestSampleSize=nrow(processed.scaled.train.B),cp=0.01)
#choose the optimal complexity penalty based on cross validation error
opcpid <- which.min(CTtree$cp[,'xerror'])
opcp <- CTtree$cp[opcpid,1]
tree_honest_CT_prune <- prune(CTtree, cp = opcp)
#plot CV-tunes optimal honest tree
print(tree_honest_CT_prune)
CTtree <- honest.causalTree(as.formula(paste("Y~",sumx)),
data=processed.scaled.train.A, treatment=processed.scaled.train.A$W,
est_data = processed.scaled.train.B, est_treatment = processed.scaled.train.B$W,
split.Rule=split.Rule.temp,split.Honest=split.Honest.temp,
split.Bucket=split.Bucket.temp, bucketNum = bucketNum.temp, bucketMax = bucketMax.temp,
cv.option=cv.option.temp, cv.Honest=cv.Honest.temp,
minsize = minsize.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
HonestSampleSize=nrow(processed.scaled.train.B),cp=0.00)
#choose the optimal complexity penalty based on cross validation error
opcpid <- which.min(CTtree$cp[,'xerror'])
opcp <- CTtree$cp[opcpid,1]
tree_honest_CT_prune <- prune(CTtree, cp = opcp)
#plot CV-tunes optimal honest tree
print(tree_honest_CT_prune)
cf <- causalForest(cf.formula, data=processed.scaled.train,
treatment=processed.scaled.train$W,num.trees=100,
double.Sample = T, split.Bucket=F,
sample.size.total = floor(nrow(processed.scaled.train)/20),
sample.size.train.frac = .5,minsize = 5,
cv.option=cv.option.temp, cv.Honest=cv.Honest.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
ncolx = ncolx,ncov_sample=ncolx,mtry=ncolx,cp=0.00)
?double.sample
?causalForest
cf <- causalForest(cf.formula, data=processed.scaled.train,
treatment=processed.scaled.train$W,num.trees=100,
double.Sample = F, split.Bucket=F,
sample.size.total = floor(nrow(processed.scaled.train)/20),
sample.size.train.frac = .5,minsize = 5,
cv.option=cv.option.temp, cv.Honest=cv.Honest.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
ncolx = ncolx,ncov_sample=ncolx,mtry=ncolx)
cf <- causalForest(cf.formula, data=processed.scaled.train,
treatment=processed.scaled.train$W,num.trees=100,
double.Sample = F, split.Bucket=F,
sample.size.total = floor(nrow(processed.scaled.train)/20),
sample.size.train.frac = .5,minsize = 5,
cv.option=cv.option.temp, cv.Honest=cv.Honest.temp,
split.alpha = split.alpha.temp, cv.alpha = cv.alpha.temp,
split.Rule=split.Rule.temp, split.Honest=split.Honest.temp,
ncolx = ncolx,ncov_sample=ncov_sample)
prp(cf$trees[[1]])
prp(cf$trees[[3]])
prp(cf$trees[[10]])
prp(cf$trees[[15]])
prp(cf$trees[[19]])
prp(cf$trees[[20]])
prp(cf$trees[[21]])
prp(cf$trees[[22]])
prp(cf$trees[[09]])
prp(cf$trees[[100]])
prp(cf$trees[[90]])
prp(cf$trees[[70]])
prp(cf$trees[[65]])
print(cf$trees[[65]])
print(cf$trees[[63]])
print(cf$trees[[64]])
print(cf$trees[[61]])
